{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swarooptest/DOMAIN_EXTRACTOR_COMMON_CRAWL/blob/main/complete_script_common_crawl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX15vNM29TJt"
      },
      "source": [
        "# COMMON_CRAWL https://commoncrawl.org/get-started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFvrC0189cNr"
      },
      "source": [
        "**BASE URL FORMAT TO DOWNLOAD THE INDEXES/CDX FILES**                \n",
        "                    https://data.commoncrawl.org/crawl-data/CC-MAIN-2019-04/cc-index.paths.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA6uiAy7965s"
      },
      "source": [
        "**EXTRACT THE CDX URLS FROM THE BASE FILE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwqNecxC7oHf",
        "outputId": "a2fec565-659a-4141-b056-18af02a013cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2019-04/cc-index.paths.gz to cc-index.paths.gz...\n",
            "Download complete: cc-index.paths.gz\n",
            "Found 302 URLs in the cc-index.paths.gz file\n",
            "\n",
            "First 5 URLs (examples):\n",
            "1. https://data.commoncrawl.org/cc-index/collections/CC-MAIN-2019-04/indexes/cdx-00000.gz\n",
            "2. https://data.commoncrawl.org/cc-index/collections/CC-MAIN-2019-04/indexes/cdx-00001.gz\n",
            "3. https://data.commoncrawl.org/cc-index/collections/CC-MAIN-2019-04/indexes/cdx-00002.gz\n",
            "4. https://data.commoncrawl.org/cc-index/collections/CC-MAIN-2019-04/indexes/cdx-00003.gz\n",
            "5. https://data.commoncrawl.org/cc-index/collections/CC-MAIN-2019-04/indexes/cdx-00004.gz\n",
            "URLs saved to CC-MAIN-2019-04/CC-MAIN-2019-04_CC-INDEXES_CDX.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import gzip\n",
        "import csv\n",
        "import urllib.request\n",
        "import os\n",
        "\n",
        "def download_file(url, local_filename=None):\n",
        "    \"\"\"Download a file from a URL to a local file.\"\"\"\n",
        "    if local_filename is None:\n",
        "        local_filename = url.split('/')[-1]\n",
        "\n",
        "    print(f\"Downloading {url} to {local_filename}...\")\n",
        "\n",
        "    if os.path.exists(local_filename):\n",
        "        print(f\"File {local_filename} already exists, skipping download.\")\n",
        "        return local_filename\n",
        "\n",
        "    try:\n",
        "        with urllib.request.urlopen(url) as response, open(local_filename, 'wb') as out_file:\n",
        "            out_file.write(response.read())\n",
        "        print(f\"Download complete: {local_filename}\")\n",
        "        return local_filename\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading file: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_urls(paths_file):\n",
        "    \"\"\"Extract URLs from the cc-index.paths.gz file.\"\"\"\n",
        "    base_url = \"https://data.commoncrawl.org\"\n",
        "    urls = []\n",
        "\n",
        "    try:\n",
        "        with gzip.open(paths_file, 'rt', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    # Create the full URL by combining the base URL with the path\n",
        "                    full_url = f\"{base_url}/{line}\"\n",
        "                    urls.append(full_url)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file: {e}\")\n",
        "\n",
        "    return urls\n",
        "\n",
        "def save_to_csv(urls, csv_filename=\"cc_index_urls.csv\"):\n",
        "    \"\"\"Save URLs to a CSV file.\"\"\"\n",
        "    try:\n",
        "        # Create directory if it doesn't exist\n",
        "        os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
        "\n",
        "        with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow(['URL'])  # Header\n",
        "            for url in urls:\n",
        "                writer.writerow([url])\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving to CSV: {e}\")\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    # URL of the cc-index.paths.gz file\n",
        "    paths_url = \"https://data.commoncrawl.org/crawl-data/CC-MAIN-2019-04/cc-index.paths.gz\"\n",
        "\n",
        "    # Extract crawl ID from URL (e.g., \"CC-MAIN-2019-04\")\n",
        "    import re\n",
        "    crawl_id_match = re.search(r'(CC-MAIN-\\d{4}-\\d{2})', paths_url)\n",
        "    crawl_id = crawl_id_match.group(1) if crawl_id_match else \"unknown-crawl\"\n",
        "\n",
        "    # Local filename to save the downloaded file\n",
        "    local_file = \"cc-index.paths.gz\"\n",
        "\n",
        "    # Download the file\n",
        "    downloaded_file = download_file(paths_url, local_file)\n",
        "\n",
        "    if downloaded_file:\n",
        "        # Extract URLs from the file\n",
        "        urls = extract_urls(downloaded_file)\n",
        "\n",
        "        # Count and print the number of URLs\n",
        "        url_count = len(urls)\n",
        "        print(f\"Found {url_count} URLs in the cc-index.paths.gz file\")\n",
        "\n",
        "        # Print the first few URLs as examples\n",
        "        if url_count > 0:\n",
        "            print(\"\\nFirst 5 URLs (examples):\")\n",
        "            for i, url in enumerate(urls[:5]):\n",
        "                print(f\"{i+1}. {url}\")\n",
        "\n",
        "        # Create directory for crawl ID if it doesn't exist\n",
        "        os.makedirs(crawl_id, exist_ok=True)\n",
        "\n",
        "        # Save URLs to CSV with dynamic name based on crawl ID\n",
        "        csv_filename = f\"{crawl_id}/{crawl_id}_CC-INDEXES_CDX.csv\"\n",
        "        save_to_csv(urls, csv_filename)\n",
        "        print(f\"URLs saved to {csv_filename}\")\n",
        "    else:\n",
        "        print(\"Failed to download or process the file\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni4aoKjHA6R8"
      },
      "source": [
        "**EXTRACT DOMAIN AND SUBDOMAIN FROM THE CDX URL \"SINGLE URL\"**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        },
        "id": "nALsx1jHDvoJ",
        "outputId": "69457122-187d-45be-b44b-5fb418cdc780"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://data.commoncrawl.org/cc-index/collections/CC-MAIN-2016-07/indexes/cdx-00045.gz\n",
            "Download completed in 5.61 seconds\n",
            "Extracting domains...\n",
            "Processed 100000 lines, found 1554 unique domains so far\n",
            "Processed 200000 lines, found 4830 unique domains so far\n",
            "Processed 300000 lines, found 6495 unique domains so far\n",
            "Processed 400000 lines, found 6552 unique domains so far\n",
            "Processed 500000 lines, found 6552 unique domains so far\n",
            "Processed 600000 lines, found 7081 unique domains so far\n",
            "Processed 700000 lines, found 8308 unique domains so far\n",
            "Processed 800000 lines, found 8909 unique domains so far\n",
            "Processed 900000 lines, found 9622 unique domains so far\n",
            "Processed 1000000 lines, found 9622 unique domains so far\n",
            "Processed 1100000 lines, found 9622 unique domains so far\n",
            "Processed 1200000 lines, found 9629 unique domains so far\n",
            "Processed 1300000 lines, found 9629 unique domains so far\n",
            "Processed 1400000 lines, found 9629 unique domains so far\n",
            "Processed 1500000 lines, found 10369 unique domains so far\n",
            "Processed 1600000 lines, found 12417 unique domains so far\n",
            "Processed 1700000 lines, found 12538 unique domains so far\n",
            "Processed 1800000 lines, found 12538 unique domains so far\n",
            "Processed 1900000 lines, found 12538 unique domains so far\n",
            "Processed 2000000 lines, found 12878 unique domains so far\n",
            "Processed 2100000 lines, found 12878 unique domains so far\n",
            "Processed 2200000 lines, found 12878 unique domains so far\n",
            "Processed 2300000 lines, found 12878 unique domains so far\n",
            "Processed 2400000 lines, found 13226 unique domains so far\n",
            "Processed 2500000 lines, found 13382 unique domains so far\n",
            "Processed 2600000 lines, found 13382 unique domains so far\n",
            "Processed 2700000 lines, found 13382 unique domains so far\n",
            "Processed 2800000 lines, found 13382 unique domains so far\n",
            "Processed 2900000 lines, found 13416 unique domains so far\n",
            "Processed 3000000 lines, found 13416 unique domains so far\n",
            "Processed 3100000 lines, found 13416 unique domains so far\n",
            "Processed 3200000 lines, found 13416 unique domains so far\n",
            "Processed 3300000 lines, found 13416 unique domains so far\n",
            "Processed 3400000 lines, found 13416 unique domains so far\n",
            "Processed 3500000 lines, found 13416 unique domains so far\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d06880c761e6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;31m# Extract domains from the specified CDX file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0mextract_domains_from_direct_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcdx_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_limit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-d06880c761e6>\u001b[0m in \u001b[0;36mextract_domains_from_direct_url\u001b[0;34m(url, output_file, display_limit)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extracting domains...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mdomains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_cdx_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mprocessing_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-d06880c761e6>\u001b[0m in \u001b[0;36mprocess_cdx_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;31m# Split the line into SURT, timestamp, and JSON parts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/gzip.py\u001b[0m in \u001b[0;36mread1\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    519\u001b[0m                                \"end-of-stream marker was reached\")\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_read_data\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0muncompress\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muncompress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0muncompress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/gzip.py\u001b[0m in \u001b[0;36m_add_read_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_read_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_crc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrc32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_crc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import gzip\n",
        "import json\n",
        "import urllib.request\n",
        "import os\n",
        "from urllib.parse import urlparse\n",
        "import time\n",
        "import re\n",
        "import csv\n",
        "import socket\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def extract_domains_from_direct_url(url, output_file, display_limit=20):\n",
        "    \"\"\"\n",
        "    Extract domain and subdomain information from a direct Common Crawl CDX file URL.\n",
        "\n",
        "    Args:\n",
        "        url: Direct URL to the CDX file\n",
        "        output_file: Path to output file to save domains\n",
        "        display_limit: Number of domains to display in console\n",
        "    \"\"\"\n",
        "    domains = set()\n",
        "\n",
        "    try:\n",
        "        # Create a temporary file to store the downloaded gzipped content\n",
        "        temp_file = \"temp_cc_file.gz\"\n",
        "\n",
        "        # Download the file\n",
        "        print(f\"Downloading {url}\")\n",
        "        start_time = time.time()\n",
        "        urllib.request.urlretrieve(url, temp_file)\n",
        "        download_time = time.time() - start_time\n",
        "        print(f\"Download completed in {download_time:.2f} seconds\")\n",
        "\n",
        "        # Process the downloaded file\n",
        "        print(\"Extracting domains...\")\n",
        "        start_time = time.time()\n",
        "        domains = process_cdx_file(temp_file)\n",
        "        processing_time = time.time() - start_time\n",
        "\n",
        "        print(f\"Extracted {len(domains)} unique domains\")\n",
        "        print(f\"Processing completed in {processing_time:.2f} seconds\")\n",
        "\n",
        "        # Display some of the domains found\n",
        "        if domains:\n",
        "            print(f\"\\nSample of domains found (showing up to {display_limit}):\")\n",
        "            for i, domain in enumerate(sorted(domains)):\n",
        "                if i >= display_limit:\n",
        "                    print(f\"...and {len(domains) - display_limit} more\")\n",
        "                    break\n",
        "                print(f\"  - {domain}\")\n",
        "            print()\n",
        "\n",
        "        # Remove the temporary file\n",
        "        os.remove(temp_file)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {url}: {e}\")\n",
        "\n",
        "    # Create the CSV file with domain and subdomain information\n",
        "    create_domain_csv(domains, output_file)\n",
        "\n",
        "    # Display some structured domains\n",
        "    print(\"\\n=== SAMPLE OF DOMAINS AND SUBDOMAINS ===\")\n",
        "    display_sample_from_csv(output_file, limit=display_limit)\n",
        "\n",
        "    print(f\"\\nCompleted! Found {len(domains)} unique domains.\")\n",
        "\n",
        "    return domains\n",
        "\n",
        "def process_cdx_file(file_path):\n",
        "    \"\"\"Process a single CDX file and return a set of domains\"\"\"\n",
        "    domains = set()\n",
        "\n",
        "    with gzip.open(file_path, 'rt', errors='replace') as f:\n",
        "        for line_number, line in enumerate(f):\n",
        "            try:\n",
        "                # Split the line into SURT, timestamp, and JSON parts\n",
        "                line = line.strip()\n",
        "\n",
        "                # Find the JSON part - it starts after the second space\n",
        "                parts = line.split(' ', 2)\n",
        "                if len(parts) < 3:\n",
        "                    continue\n",
        "\n",
        "                surt = parts[0]\n",
        "                json_str = parts[2]\n",
        "\n",
        "                # Extract domain from JSON\n",
        "                try:\n",
        "                    record = json.loads(json_str)\n",
        "                    url = record.get('url', '')\n",
        "\n",
        "                    if url:\n",
        "                        parsed_url = urlparse(url)\n",
        "                        domain = parsed_url.netloc\n",
        "\n",
        "                        if domain:\n",
        "                            domains.add(domain)\n",
        "                except json.JSONDecodeError:\n",
        "                    # If JSON parsing fails, try to extract domain from SURT\n",
        "                    match = re.match(r'^([^)]+)\\)', surt)\n",
        "                    if match:\n",
        "                        surt_domain = match.group(1)\n",
        "                        # Convert SURT domain to normal domain\n",
        "                        domain_parts = surt_domain.split(',')\n",
        "                        domain = '.'.join(reversed(domain_parts))\n",
        "                        domains.add(domain)\n",
        "            except Exception as e:\n",
        "                if line_number % 100000 == 0:  # Only show occasional errors to reduce output\n",
        "                    print(f\"Error processing line {line_number}: {e}\")\n",
        "                continue\n",
        "\n",
        "            # Show progress\n",
        "            if line_number > 0 and line_number % 100000 == 0:\n",
        "                print(f\"Processed {line_number} lines, found {len(domains)} unique domains so far\")\n",
        "\n",
        "    return domains\n",
        "\n",
        "def split_domain_parts(domain):\n",
        "    \"\"\"\n",
        "    Split a domain into main domain and subdomain parts.\n",
        "    Handles common multi-level TLDs.\n",
        "    \"\"\"\n",
        "    parts = domain.split('.')\n",
        "\n",
        "    # Handle common multi-part TLDs\n",
        "    common_tlds = ['co.uk', 'com.au', 'co.nz', 'co.jp', 'co.za', 'ac.uk', 'gov.au', 'edu.au']\n",
        "\n",
        "    if len(parts) >= 3:\n",
        "        potential_tld = f\"{parts[-2]}.{parts[-1]}\"\n",
        "        if potential_tld in common_tlds:\n",
        "            # It's a multi-part TLD\n",
        "            if len(parts) > 3:  # Has subdomain\n",
        "                main_domain = f\"{parts[-3]}.{parts[-2]}.{parts[-1]}\"\n",
        "                subdomain = '.'.join(parts[:-3])\n",
        "                return main_domain, subdomain\n",
        "            else:\n",
        "                return f\"{parts[-3]}.{parts[-2]}.{parts[-1]}\", \"\"\n",
        "        else:\n",
        "            # Regular domain\n",
        "            if len(parts) > 2:  # Has subdomain\n",
        "                main_domain = f\"{parts[-2]}.{parts[-1]}\"\n",
        "                subdomain = '.'.join(parts[:-2])\n",
        "                return main_domain, subdomain\n",
        "            else:\n",
        "                return f\"{parts[-2]}.{parts[-1]}\", \"\"\n",
        "    elif len(parts) == 2:\n",
        "        # Just domain.tld\n",
        "        return domain, \"\"\n",
        "    else:\n",
        "        # Invalid domain\n",
        "        return domain, \"\"\n",
        "\n",
        "def get_ip_address(domain):\n",
        "    \"\"\"Resolve domain to IP address\"\"\"\n",
        "    try:\n",
        "        ip = socket.gethostbyname(domain)\n",
        "        return ip\n",
        "    except socket.gaierror:\n",
        "        return \"Could not resolve\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "def resolve_domain_batch(domains, max_workers=100):\n",
        "    \"\"\"Resolve a batch of domains to IP addresses using threading\"\"\"\n",
        "    domain_to_ip = {}\n",
        "\n",
        "    print(f\"Resolving IP addresses for {len(domains)} domains (this may take some time)...\")\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # Create a mapping of futures to domains\n",
        "        future_to_domain = {executor.submit(get_ip_address, domain): domain for domain in domains}\n",
        "\n",
        "        # Process as they complete\n",
        "        completed = 0\n",
        "        for future in future_to_domain:\n",
        "            domain = future_to_domain[future]\n",
        "            try:\n",
        "                ip = future.result()\n",
        "                domain_to_ip[domain] = ip\n",
        "            except Exception as e:\n",
        "                domain_to_ip[domain] = f\"Error: {str(e)}\"\n",
        "\n",
        "            completed += 1\n",
        "            if completed % 100 == 0:\n",
        "                print(f\"Resolved {completed}/{len(domains)} domains\")\n",
        "\n",
        "    return domain_to_ip\n",
        "\n",
        "def create_domain_csv(domains, output_file):\n",
        "    \"\"\"Create a CSV with domain, subdomain and IP address\"\"\"\n",
        "    print(f\"Creating CSV file: {output_file}\")\n",
        "\n",
        "    # Ensure directory exists\n",
        "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
        "\n",
        "    # Resolve IP addresses for all domains\n",
        "    domain_to_ip = resolve_domain_batch(domains)\n",
        "\n",
        "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['Full Domain', 'Main Domain', 'Subdomain', 'IP Address'])\n",
        "\n",
        "        for full_domain in sorted(domains):\n",
        "            main_domain, subdomain = split_domain_parts(full_domain)\n",
        "            ip_address = domain_to_ip.get(full_domain, \"Could not resolve\")\n",
        "            writer.writerow([full_domain, main_domain, subdomain, ip_address])\n",
        "\n",
        "    print(f\"CSV file created successfully with {len(domains)} entries\")\n",
        "\n",
        "def display_sample_from_csv(csv_file, limit=20):\n",
        "    \"\"\"Display a sample of domains from the CSV file\"\"\"\n",
        "    try:\n",
        "        with open(csv_file, 'r', newline='', encoding='utf-8') as f:\n",
        "            reader = csv.reader(f)\n",
        "            header = next(reader)  # Skip header\n",
        "\n",
        "            count = 0\n",
        "            for row in reader:\n",
        "                if count >= limit:\n",
        "                    print(f\"...and more entries (showing {limit} of total)\")\n",
        "                    break\n",
        "\n",
        "                full_domain, main_domain, subdomain, ip_address = row\n",
        "\n",
        "                print(f\"Domain: {main_domain}\")\n",
        "                if subdomain:\n",
        "                    print(f\"  Subdomain: {subdomain}\")\n",
        "                print(f\"  IP Address: {ip_address}\")\n",
        "                print()\n",
        "\n",
        "                count += 1\n",
        "    except Exception as e:\n",
        "        print(f\"Error displaying CSV contents: {e}\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Direct URL to the CDX file\n",
        "    cdx_url = \"https://data.commoncrawl.org/cc-index/collections/CC-MAIN-2016-07/indexes/cdx-00045.gz\"\n",
        "\n",
        "    # Extract crawl ID from URL for dynamic output file naming\n",
        "    import re\n",
        "    crawl_id_match = re.search(r'(CC-MAIN-\\d{4}-\\d{2})', cdx_url)\n",
        "    crawl_id = crawl_id_match.group(1) if crawl_id_match else \"unknown-crawl\"\n",
        "\n",
        "    # Create directory for crawl ID if it doesn't exist\n",
        "    os.makedirs(crawl_id, exist_ok=True)\n",
        "\n",
        "    # Output file with dynamic naming based on crawl ID\n",
        "    output_file = f\"{crawl_id}/domains.csv\"\n",
        "\n",
        "    # Number of domains to display in console\n",
        "    display_limit = 20\n",
        "\n",
        "    # Extract domains from the specified CDX file\n",
        "    extract_domains_from_direct_url(cdx_url, output_file, display_limit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm4JmHHpf3Am"
      },
      "source": [
        "# Process URLs from a CSV file with column name \"URL\"\n",
        "Handle errors gracefully\n",
        "\n",
        "Auto-delete downloaded files after processing\n",
        "\n",
        "Store extracted data from each file separately with proper naming\n",
        "\n",
        "Allow the script to resume from where it left off if interrupted\n",
        "\n",
        "Use a unique name based on the URL for resumption purposes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5TzZpVrf3eM"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "import json\n",
        "import urllib.request\n",
        "import os\n",
        "from urllib.parse import urlparse\n",
        "import time\n",
        "import re\n",
        "import csv\n",
        "import socket\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import argparse\n",
        "import hashlib\n",
        "import logging\n",
        "import sys\n",
        "from google.colab import files  # For Colab file upload/download\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"domain_extractor.log\"),\n",
        "        logging.StreamHandler(sys.stdout)\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def create_progress_tracker(csv_file):\n",
        "    \"\"\"\n",
        "    Create a progress tracker file to keep track of processed URLs\n",
        "\n",
        "    Args:\n",
        "        csv_file: Path to the input CSV file\n",
        "\n",
        "    Returns:\n",
        "        Path to the progress tracker file\n",
        "    \"\"\"\n",
        "    base_name = os.path.basename(csv_file)\n",
        "    name_without_ext = os.path.splitext(base_name)[0]\n",
        "    tracker_file = f\"{name_without_ext}_progress.txt\"\n",
        "\n",
        "    # Create the file if it doesn't exist\n",
        "    if not os.path.exists(tracker_file):\n",
        "        with open(tracker_file, 'w') as f:\n",
        "            f.write(\"# URLs that have been processed\\n\")\n",
        "\n",
        "    return tracker_file\n",
        "\n",
        "def get_processed_urls(tracker_file):\n",
        "    \"\"\"\n",
        "    Get the list of URLs that have already been processed\n",
        "\n",
        "    Args:\n",
        "        tracker_file: Path to the progress tracker file\n",
        "\n",
        "    Returns:\n",
        "        Set of URLs that have been processed\n",
        "    \"\"\"\n",
        "    processed_urls = set()\n",
        "\n",
        "    if os.path.exists(tracker_file):\n",
        "        with open(tracker_file, 'r') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line and not line.startswith('#'):\n",
        "                    processed_urls.add(line)\n",
        "\n",
        "    return processed_urls\n",
        "\n",
        "def mark_url_as_processed(tracker_file, url):\n",
        "    \"\"\"\n",
        "    Mark a URL as processed in the tracker file\n",
        "\n",
        "    Args:\n",
        "        tracker_file: Path to the progress tracker file\n",
        "        url: URL that has been processed\n",
        "    \"\"\"\n",
        "    with open(tracker_file, 'a') as f:\n",
        "        f.write(f\"{url}\\n\")\n",
        "\n",
        "def generate_url_filename(url):\n",
        "    \"\"\"\n",
        "    Generate a unique filename based on the URL\n",
        "\n",
        "    Args:\n",
        "        url: URL to generate filename for\n",
        "\n",
        "    Returns:\n",
        "        A unique filename based on the URL\n",
        "    \"\"\"\n",
        "    # Extract crawl ID and index number if possible\n",
        "    crawl_id_match = re.search(r'(CC-MAIN-\\d{4}-\\d{2})', url)\n",
        "    index_match = re.search(r'cdx-(\\d+)\\.gz', url)\n",
        "\n",
        "    if crawl_id_match and index_match:\n",
        "        crawl_id = crawl_id_match.group(1)\n",
        "        index_num = index_match.group(1)\n",
        "        return f\"{crawl_id}_cdx_{index_num}\"\n",
        "\n",
        "    # Fallback to a hash of the URL\n",
        "    url_hash = hashlib.md5(url.encode()).hexdigest()[:8]\n",
        "    parsed_url = urlparse(url)\n",
        "    path_parts = parsed_url.path.split('/')\n",
        "    file_part = path_parts[-1] if path_parts[-1] else 'index'\n",
        "\n",
        "    return f\"{parsed_url.netloc.replace('.', '_')}_{file_part}_{url_hash}\"\n",
        "\n",
        "def extract_domains_from_direct_url(url, output_file, display_limit=20):\n",
        "    \"\"\"\n",
        "    Extract domain and subdomain information from a direct Common Crawl CDX file URL.\n",
        "\n",
        "    Args:\n",
        "        url: Direct URL to the CDX file\n",
        "        output_file: Path to output file to save domains\n",
        "        display_limit: Number of domains to display in console\n",
        "\n",
        "    Returns:\n",
        "        True if processing was successful, False otherwise\n",
        "    \"\"\"\n",
        "    domains = set()\n",
        "    temp_file = None\n",
        "    success = False\n",
        "\n",
        "    try:\n",
        "        # Create a temporary file to store the downloaded gzipped content\n",
        "        temp_file = f\"temp_cc_file_{int(time.time())}.gz\"\n",
        "\n",
        "        # Download the file\n",
        "        logger.info(f\"Downloading {url}\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            urllib.request.urlretrieve(url, temp_file)\n",
        "            download_time = time.time() - start_time\n",
        "            logger.info(f\"Download completed in {download_time:.2f} seconds\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to download {url}: {e}\")\n",
        "            return False\n",
        "\n",
        "        # Process the downloaded file\n",
        "        logger.info(\"Extracting domains...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            domains = process_cdx_file(temp_file)\n",
        "            processing_time = time.time() - start_time\n",
        "\n",
        "            logger.info(f\"Extracted {len(domains)} unique domains\")\n",
        "            logger.info(f\"Processing completed in {processing_time:.2f} seconds\")\n",
        "\n",
        "            # Display some of the domains found\n",
        "            if domains:\n",
        "                logger.info(f\"\\nSample of domains found (showing up to {display_limit}):\")\n",
        "                for i, domain in enumerate(sorted(domains)):\n",
        "                    if i >= display_limit:\n",
        "                        logger.info(f\"...and {len(domains) - display_limit} more\")\n",
        "                        break\n",
        "                    logger.info(f\"  - {domain}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to process file {temp_file}: {e}\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing {url}: {e}\")\n",
        "        return False\n",
        "\n",
        "    finally:\n",
        "        # Remove the temporary file regardless of success or failure\n",
        "        if temp_file and os.path.exists(temp_file):\n",
        "            try:\n",
        "                os.remove(temp_file)\n",
        "                logger.info(f\"Deleted temporary file: {temp_file}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to delete temporary file {temp_file}: {e}\")\n",
        "\n",
        "    # Create the CSV file with domain and subdomain information\n",
        "    if domains:\n",
        "        try:\n",
        "            create_domain_csv(domains, output_file)\n",
        "\n",
        "            # Display some structured domains\n",
        "            logger.info(\"\\n=== SAMPLE OF DOMAINS AND SUBDOMAINS ===\")\n",
        "            display_sample_from_csv(output_file, limit=display_limit)\n",
        "\n",
        "            logger.info(f\"\\nCompleted! Found {len(domains)} unique domains.\")\n",
        "            success = True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to create CSV file {output_file}: {e}\")\n",
        "            return False\n",
        "    else:\n",
        "        logger.warning(\"No domains were extracted. Output file not created.\")\n",
        "        return False\n",
        "\n",
        "    return success\n",
        "\n",
        "def process_cdx_file(file_path):\n",
        "    \"\"\"Process a single CDX file and return a set of domains\"\"\"\n",
        "    domains = set()\n",
        "\n",
        "    with gzip.open(file_path, 'rt', errors='replace') as f:\n",
        "        for line_number, line in enumerate(f):\n",
        "            try:\n",
        "                # Split the line into SURT, timestamp, and JSON parts\n",
        "                line = line.strip()\n",
        "\n",
        "                # Find the JSON part - it starts after the second space\n",
        "                parts = line.split(' ', 2)\n",
        "                if len(parts) < 3:\n",
        "                    continue\n",
        "\n",
        "                surt = parts[0]\n",
        "                json_str = parts[2]\n",
        "\n",
        "                # Extract domain from JSON\n",
        "                try:\n",
        "                    record = json.loads(json_str)\n",
        "                    url = record.get('url', '')\n",
        "\n",
        "                    if url:\n",
        "                        parsed_url = urlparse(url)\n",
        "                        domain = parsed_url.netloc\n",
        "\n",
        "                        if domain:\n",
        "                            domains.add(domain)\n",
        "                except json.JSONDecodeError:\n",
        "                    # If JSON parsing fails, try to extract domain from SURT\n",
        "                    match = re.match(r'^([^)]+)\\)', surt)\n",
        "                    if match:\n",
        "                        surt_domain = match.group(1)\n",
        "                        # Convert SURT domain to normal domain\n",
        "                        domain_parts = surt_domain.split(',')\n",
        "                        domain = '.'.join(reversed(domain_parts))\n",
        "                        domains.add(domain)\n",
        "            except Exception as e:\n",
        "                if line_number % 100000 == 0:  # Only show occasional errors to reduce output\n",
        "                    logger.warning(f\"Error processing line {line_number}: {e}\")\n",
        "                continue\n",
        "\n",
        "            # Show progress\n",
        "            if line_number > 0 and line_number % 100000 == 0:\n",
        "                logger.info(f\"Processed {line_number} lines, found {len(domains)} unique domains so far\")\n",
        "\n",
        "    return domains\n",
        "\n",
        "def split_domain_parts(domain):\n",
        "    \"\"\"\n",
        "    Split a domain into main domain and subdomain parts.\n",
        "    Handles common multi-level TLDs.\n",
        "    \"\"\"\n",
        "    parts = domain.split('.')\n",
        "\n",
        "    # Handle common multi-part TLDs\n",
        "    common_tlds = ['co.uk', 'com.au', 'co.nz', 'co.jp', 'co.za', 'ac.uk', 'gov.au', 'edu.au']\n",
        "\n",
        "    if len(parts) >= 3:\n",
        "        potential_tld = f\"{parts[-2]}.{parts[-1]}\"\n",
        "        if potential_tld in common_tlds:\n",
        "            # It's a multi-part TLD\n",
        "            if len(parts) > 3:  # Has subdomain\n",
        "                main_domain = f\"{parts[-3]}.{parts[-2]}.{parts[-1]}\"\n",
        "                subdomain = '.'.join(parts[:-3])\n",
        "                return main_domain, subdomain\n",
        "            else:\n",
        "                return f\"{parts[-3]}.{parts[-2]}.{parts[-1]}\", \"\"\n",
        "        else:\n",
        "            # Regular domain\n",
        "            if len(parts) > 2:  # Has subdomain\n",
        "                main_domain = f\"{parts[-2]}.{parts[-1]}\"\n",
        "                subdomain = '.'.join(parts[:-2])\n",
        "                return main_domain, subdomain\n",
        "            else:\n",
        "                return f\"{parts[-2]}.{parts[-1]}\", \"\"\n",
        "    elif len(parts) == 2:\n",
        "        # Just domain.tld\n",
        "        return domain, \"\"\n",
        "    else:\n",
        "        # Invalid domain\n",
        "        return domain, \"\"\n",
        "\n",
        "def get_ip_address(domain):\n",
        "    \"\"\"Resolve domain to IP address with error handling\"\"\"\n",
        "    try:\n",
        "        ip = socket.gethostbyname(domain)\n",
        "        return ip\n",
        "    except socket.gaierror:\n",
        "        return \"Could not resolve\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "def resolve_domain_batch(domains, max_workers=100):\n",
        "    \"\"\"Resolve a batch of domains to IP addresses using threading with error handling\"\"\"\n",
        "    domain_to_ip = {}\n",
        "\n",
        "    logger.info(f\"Resolving IP addresses for {len(domains)} domains (this may take some time)...\")\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # Create a mapping of futures to domains\n",
        "        future_to_domain = {executor.submit(get_ip_address, domain): domain for domain in domains}\n",
        "\n",
        "        # Process as they complete\n",
        "        completed = 0\n",
        "        for future in future_to_domain:\n",
        "            domain = future_to_domain[future]\n",
        "            try:\n",
        "                ip = future.result()\n",
        "                domain_to_ip[domain] = ip\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error resolving {domain}: {e}\")\n",
        "                domain_to_ip[domain] = f\"Error: {str(e)}\"\n",
        "\n",
        "            completed += 1\n",
        "            if completed % 100 == 0:\n",
        "                logger.info(f\"Resolved {completed}/{len(domains)} domains\")\n",
        "\n",
        "    return domain_to_ip\n",
        "\n",
        "def create_domain_csv(domains, output_file):\n",
        "    \"\"\"Create a CSV with domain, subdomain and IP address\"\"\"\n",
        "    logger.info(f\"Creating CSV file: {output_file}\")\n",
        "\n",
        "    # Ensure directory exists\n",
        "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
        "\n",
        "    # Resolve IP addresses for all domains\n",
        "    domain_to_ip = resolve_domain_batch(domains)\n",
        "\n",
        "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['Full Domain', 'Main Domain', 'Subdomain', 'IP Address'])\n",
        "\n",
        "        for full_domain in sorted(domains):\n",
        "            main_domain, subdomain = split_domain_parts(full_domain)\n",
        "            ip_address = domain_to_ip.get(full_domain, \"Could not resolve\")\n",
        "            writer.writerow([full_domain, main_domain, subdomain, ip_address])\n",
        "\n",
        "    logger.info(f\"CSV file created successfully with {len(domains)} entries\")\n",
        "\n",
        "def display_sample_from_csv(csv_file, limit=20):\n",
        "    \"\"\"Display a sample of domains from the CSV file\"\"\"\n",
        "    try:\n",
        "        with open(csv_file, 'r', newline='', encoding='utf-8') as f:\n",
        "            reader = csv.reader(f)\n",
        "            header = next(reader)  # Skip header\n",
        "\n",
        "            count = 0\n",
        "            for row in reader:\n",
        "                if count >= limit:\n",
        "                    logger.info(f\"...and more entries (showing {limit} of total)\")\n",
        "                    break\n",
        "\n",
        "                full_domain, main_domain, subdomain, ip_address = row\n",
        "\n",
        "                logger.info(f\"Domain: {main_domain}\")\n",
        "                if subdomain:\n",
        "                    logger.info(f\"  Subdomain: {subdomain}\")\n",
        "                logger.info(f\"  IP Address: {ip_address}\")\n",
        "                logger.info(\"\")\n",
        "\n",
        "                count += 1\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error displaying CSV contents: {e}\")\n",
        "\n",
        "def process_urls_from_csv(input_csv, output_dir=\"results\", display_limit=20, start_url=None):\n",
        "    \"\"\"\n",
        "    Process multiple URLs from a CSV file with resumption capability.\n",
        "\n",
        "    Args:\n",
        "        input_csv: Path to input CSV file containing URLs\n",
        "        output_dir: Directory to store output files\n",
        "        display_limit: Number of domains to display in console for each URL\n",
        "        start_url: Optional URL to start processing from (for resumption)\n",
        "    \"\"\"\n",
        "    logger.info(f\"Processing URLs from {input_csv}\")\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Create progress tracker\n",
        "    tracker_file = create_progress_tracker(input_csv)\n",
        "    processed_urls = get_processed_urls(tracker_file)\n",
        "\n",
        "    if processed_urls:\n",
        "        logger.info(f\"Found {len(processed_urls)} already processed URLs in tracker file\")\n",
        "\n",
        "    # Read URLs from CSV\n",
        "    urls = []\n",
        "    start_processing = False if start_url else True\n",
        "\n",
        "    try:\n",
        "        with open(input_csv, 'r', newline='', encoding='utf-8') as f:\n",
        "            reader = csv.DictReader(f)\n",
        "\n",
        "            # Check if URL column exists\n",
        "            if \"URL\" not in reader.fieldnames:\n",
        "                logger.error(\"Column 'URL' not found in the CSV file\")\n",
        "                return\n",
        "\n",
        "            # Get URLs from the URL column\n",
        "            for row in reader:\n",
        "                url = row[\"URL\"].strip()\n",
        "                if url and url.startswith(\"http\"):\n",
        "                    urls.append(url)\n",
        "\n",
        "        logger.info(f\"Found {len(urls)} URLs to process\")\n",
        "\n",
        "        # Process each URL\n",
        "        for i, url in enumerate(urls):\n",
        "            # Check if URL has already been processed\n",
        "            if url in processed_urls:\n",
        "                logger.info(f\"[{i+1}/{len(urls)}] Skipping already processed URL: {url}\")\n",
        "                continue\n",
        "\n",
        "            # If start_url is specified, skip until we find it\n",
        "            if not start_processing:\n",
        "                if url == start_url:\n",
        "                    start_processing = True\n",
        "                    logger.info(f\"Starting from specified URL: {url}\")\n",
        "                else:\n",
        "                    logger.info(f\"[{i+1}/{len(urls)}] Skipping URL before start point: {url}\")\n",
        "                    continue\n",
        "\n",
        "            logger.info(f\"\\n[{i+1}/{len(urls)}] Processing URL: {url}\")\n",
        "\n",
        "            # Generate output filename based on URL\n",
        "            filename = generate_url_filename(url)\n",
        "            output_file = f\"{output_dir}/{filename}_domains.csv\"\n",
        "\n",
        "            # Process the URL\n",
        "            success = extract_domains_from_direct_url(url, output_file, display_limit)\n",
        "\n",
        "            if success:\n",
        "                logger.info(f\"Completed processing URL: {url}\")\n",
        "                logger.info(f\"Results saved to: {output_file}\")\n",
        "\n",
        "                # Mark URL as processed\n",
        "                mark_url_as_processed(tracker_file, url)\n",
        "            else:\n",
        "                logger.error(f\"Failed to process URL: {url}\")\n",
        "                logger.info(\"You can resume from this URL later using the --start-url parameter\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing CSV file: {e}\")\n",
        "\n",
        "def upload_csv_file():\n",
        "    \"\"\"Allow user to upload a CSV file in Colab environment\"\"\"\n",
        "    print(\"Please upload your CSV file with URLs...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        return None\n",
        "\n",
        "    # Get the first uploaded file\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    return filename\n",
        "\n",
        "# Function to create a sample CSV if needed\n",
        "def create_sample_csv(filename=\"sample_urls.csv\"):\n",
        "    \"\"\"Create a sample CSV file with URLs\"\"\"\n",
        "    logger.info(f\"Creating sample CSV file: {filename}\")\n",
        "\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['URL'])\n",
        "        writer.writerow(['https://data.commoncrawl.org/cc-index/collections/CC-MAIN-2019-04/indexes/cdx-00000.gz'])\n",
        "        writer.writerow(['https://data.commoncrawl.org/cc-index/collections/CC-MAIN-2019-04/indexes/cdx-00001.gz'])\n",
        "        writer.writerow(['https://data.commoncrawl.org/cc-index/collections/CC-MAIN-2019-04/indexes/cdx-00002.gz'])\n",
        "\n",
        "    logger.info(f\"Sample CSV file created with 3 URLs\")\n",
        "    return filename\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function for Colab execution\"\"\"\n",
        "    print(\"Common Crawl Domain Extractor\")\n",
        "    print(\"============================\")\n",
        "\n",
        "    # Options menu\n",
        "    print(\"\\nOptions:\")\n",
        "    print(\"1. Upload a CSV file with URLs\")\n",
        "    print(\"2. Create a sample CSV file\")\n",
        "    print(\"3. Specify a CSV file path\")\n",
        "\n",
        "    choice = input(\"\\nEnter your choice (1-3): \")\n",
        "\n",
        "    input_csv = None\n",
        "    if choice == '1':\n",
        "        input_csv = upload_csv_file()\n",
        "        if not input_csv:\n",
        "            print(\"No file uploaded. Exiting.\")\n",
        "            return\n",
        "    elif choice == '2':\n",
        "        input_csv = create_sample_csv()\n",
        "    elif choice == '3':\n",
        "        input_csv = input(\"Enter the path to your CSV file: \")\n",
        "    else:\n",
        "        print(\"Invalid choice. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Get other parameters\n",
        "    output_dir = input(\"Enter output directory (press Enter for 'results'): \")\n",
        "    if not output_dir:\n",
        "        output_dir = \"results\"\n",
        "\n",
        "    display_limit_str = input(\"Enter display limit for samples (press Enter for default 20): \")\n",
        "    display_limit = int(display_limit_str) if display_limit_str else 20\n",
        "\n",
        "    start_url = input(\"Enter URL to start from (optional, press Enter to start from beginning): \")\n",
        "    if not start_url:\n",
        "        start_url = None\n",
        "\n",
        "    # Process URLs\n",
        "    print(f\"\\nProcessing URLs from {input_csv}\")\n",
        "    print(f\"Results will be saved to {output_dir}\")\n",
        "\n",
        "    process_urls_from_csv(input_csv, output_dir, display_limit, start_url)\n",
        "\n",
        "    print(\"\\nProcessing complete!\")\n",
        "    print(f\"Check the {output_dir} directory for results.\")\n",
        "\n",
        "# This will run when the script is executed directly in Colab\n",
        "if __name__ == \"__main__\":\n",
        "    # If running in Colab, use the interactive main function\n",
        "    try:\n",
        "        import google.colab\n",
        "        # Running in Colab, use interactive interface\n",
        "        main()\n",
        "    except ImportError:\n",
        "        # Not running in Colab, use command line arguments\n",
        "        parser = argparse.ArgumentParser(description='Extract domains from Common Crawl CDX files')\n",
        "        parser.add_argument('--input', '-i', type=str, required=True, help='Input CSV file containing URLs in column named \"URL\"')\n",
        "        parser.add_argument('--output', '-o', type=str, help='Output directory', default='results')\n",
        "        parser.add_argument('--limit', '-l', type=int, help='Display limit for domains', default=20)\n",
        "        parser.add_argument('--start-url', '-s', type=str, help='URL to start processing from (for resumption)')\n",
        "\n",
        "        args = parser.parse_args()\n",
        "\n",
        "        logger.info(\"=== Domain Extractor ===\")\n",
        "        logger.info(f\"Input file: {args.input}\")\n",
        "        logger.info(f\"Output directory: {args.output}\")\n",
        "        logger.info(f\"Display limit: {args.limit}\")\n",
        "        if args.start_url:\n",
        "            logger.info(f\"Starting from URL: {args.start_url}\")\n",
        "\n",
        "        # Process URLs from the CSV file\n",
        "        process_urls_from_csv(args.input, args.output, args.limit, args.start_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDoFjD8CHZtX"
      },
      "source": [
        "# VALIDATE DOMAIN NAME AND IP ADDRESS FOR EACH DOWNLOADED CSV FILE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "llQbAMRDHaNv",
        "outputId": "63b57c70-2fa8-4358-9804-fd8881c33041"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ULTRA-FAST Domain & IP Security Verifier\n",
            " Optimized for 300k-400k rows with aggressive caching\n",
            "======================================================================\n",
            "\n",
            " Speed Configuration:\n",
            " Concurrent workers: 20\n",
            " Batch size: 2000\n",
            " Aggressive caching: Enabled\n",
            " Rate limit: 10 req/sec per worker\n",
            "\n",
            " Upload your CSV file:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f576497e-756b-4e53-bc4c-9b71e29d0f16\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f576497e-756b-4e53-bc4c-9b71e29d0f16\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving CC-MAIN-2019-04_cdx_00000_domains.csv to CC-MAIN-2019-04_cdx_00000_domains.csv\n",
            " Fast loading CSV...\n",
            " Loaded 340,333 rows in 0.7s\n",
            "\n",
            " CSV Columns detected:\n",
            "  1. 'Full Domain'\n",
            "  2. 'Main Domain'\n",
            "  3. 'Subdomain'\n",
            "  4. 'IP Address'\n",
            " All required columns found!\n",
            " Analyzing data for optimization...\n",
            " Unique domains: 340,333\n",
            " Unique IPs: 106,325\n",
            " Cache will save ~-106,325 API calls\n",
            "\n",
            " Processing 171 batches of 2000 rows each\n",
            " Processing batch 1/171 (2000 rows)\n",
            " Batch 1 complete: 6 rows/sec\n",
            " Overall: 0.6% (2,000/340,333) - 6 rows/sec\n",
            " ETA: 881.9 min | Cache hit rate: 1.8%\n",
            " Processing batch 2/171 (2000 rows)\n",
            " Batch 2 complete: 6 rows/sec\n",
            " Overall: 1.2% (4,000/340,333) - 6 rows/sec\n",
            " ETA: 887.0 min | Cache hit rate: 1.0%\n",
            " Processing batch 3/171 (2000 rows)\n",
            " Batch 3 complete: 7 rows/sec\n",
            " Overall: 1.8% (6,000/340,333) - 7 rows/sec\n",
            " ETA: 846.8 min | Cache hit rate: 0.7%\n",
            " Processing batch 4/171 (2000 rows)\n",
            " Batch 4 complete: 7 rows/sec\n",
            " Overall: 2.4% (8,000/340,333) - 7 rows/sec\n",
            " ETA: 836.7 min | Cache hit rate: 0.6%\n",
            " Processing batch 5/171 (2000 rows)\n",
            " Batch 5 complete: 7 rows/sec\n",
            " Overall: 2.9% (10,000/340,333) - 7 rows/sec\n",
            " ETA: 833.4 min | Cache hit rate: 0.5%\n",
            " Checkpoint: 10,000 rows saved\n",
            " Processing batch 6/171 (2000 rows)\n",
            " Batch 6 complete: 6 rows/sec\n",
            " Overall: 3.5% (12,000/340,333) - 7 rows/sec\n",
            " ETA: 839.3 min | Cache hit rate: 0.4%\n",
            " Processing batch 7/171 (2000 rows)\n",
            " Batch 7 complete: 6 rows/sec\n",
            " Overall: 4.1% (14,000/340,333) - 6 rows/sec\n",
            " ETA: 839.5 min | Cache hit rate: 0.4%\n",
            " Processing batch 8/171 (2000 rows)\n",
            " Batch 8 complete: 6 rows/sec\n",
            " Overall: 4.7% (16,000/340,333) - 6 rows/sec\n",
            " ETA: 842.3 min | Cache hit rate: 0.4%\n",
            " Processing batch 9/171 (2000 rows)\n",
            " Batch 9 complete: 6 rows/sec\n",
            " Overall: 5.3% (18,000/340,333) - 6 rows/sec\n",
            " ETA: 842.3 min | Cache hit rate: 0.4%\n",
            " Processing batch 10/171 (2000 rows)\n",
            " Batch 10 complete: 7 rows/sec\n",
            " Overall: 5.9% (20,000/340,333) - 6 rows/sec\n",
            " ETA: 831.5 min | Cache hit rate: 0.3%\n",
            " Checkpoint: 20,000 rows saved\n",
            " Processing batch 11/171 (2000 rows)\n",
            " Batch 11 complete: 7 rows/sec\n",
            " Overall: 6.5% (22,000/340,333) - 6 rows/sec\n",
            " ETA: 823.9 min | Cache hit rate: 0.3%\n",
            " Processing batch 12/171 (2000 rows)\n",
            " Batch 12 complete: 7 rows/sec\n",
            " Overall: 7.1% (24,000/340,333) - 6 rows/sec\n",
            " ETA: 814.4 min | Cache hit rate: 0.3%\n",
            " Processing batch 13/171 (2000 rows)\n",
            " Batch 13 complete: 7 rows/sec\n",
            " Overall: 7.6% (26,000/340,333) - 7 rows/sec\n",
            " ETA: 805.4 min | Cache hit rate: 0.3%\n",
            " Processing batch 14/171 (2000 rows)\n",
            " Batch 14 complete: 6 rows/sec\n",
            " Overall: 8.2% (28,000/340,333) - 6 rows/sec\n",
            " ETA: 803.3 min | Cache hit rate: 0.3%\n",
            " Processing batch 15/171 (2000 rows)\n",
            " Batch 15 complete: 6 rows/sec\n",
            " Overall: 8.8% (30,000/340,333) - 6 rows/sec\n",
            " ETA: 799.0 min | Cache hit rate: 0.3%\n",
            " Checkpoint: 30,000 rows saved\n",
            " Processing batch 16/171 (2000 rows)\n",
            " Batch 16 complete: 7 rows/sec\n",
            " Overall: 9.4% (32,000/340,333) - 7 rows/sec\n",
            " ETA: 789.6 min | Cache hit rate: 0.4%\n",
            " Processing batch 17/171 (2000 rows)\n",
            " Batch 17 complete: 6 rows/sec\n",
            " Overall: 10.0% (34,000/340,333) - 6 rows/sec\n",
            " ETA: 788.0 min | Cache hit rate: 0.4%\n",
            " Processing batch 18/171 (2000 rows)\n",
            " Batch 18 complete: 6 rows/sec\n",
            " Overall: 10.6% (36,000/340,333) - 6 rows/sec\n",
            " ETA: 786.7 min | Cache hit rate: 0.3%\n",
            " Processing batch 19/171 (2000 rows)\n",
            " Batch 19 complete: 6 rows/sec\n",
            " Overall: 11.2% (38,000/340,333) - 6 rows/sec\n",
            " ETA: 782.1 min | Cache hit rate: 0.3%\n",
            " Processing batch 20/171 (2000 rows)\n",
            " Batch 20 complete: 6 rows/sec\n",
            " Overall: 11.8% (40,000/340,333) - 6 rows/sec\n",
            " ETA: 781.1 min | Cache hit rate: 0.3%\n",
            " Checkpoint: 40,000 rows saved\n",
            " Processing batch 21/171 (2000 rows)\n",
            " Batch 21 complete: 6 rows/sec\n",
            " Overall: 12.3% (42,000/340,333) - 6 rows/sec\n",
            " ETA: 780.3 min | Cache hit rate: 0.3%\n",
            " Processing batch 22/171 (2000 rows)\n",
            " Batch 22 complete: 8 rows/sec\n",
            " Overall: 12.9% (44,000/340,333) - 6 rows/sec\n",
            " ETA: 769.1 min | Cache hit rate: 0.3%\n",
            " Processing batch 23/171 (2000 rows)\n",
            " Batch 23 complete: 7 rows/sec\n",
            " Overall: 13.5% (46,000/340,333) - 6 rows/sec\n",
            " ETA: 759.6 min | Cache hit rate: 0.3%\n",
            " Processing batch 24/171 (2000 rows)\n",
            " Batch 24 complete: 6 rows/sec\n",
            " Overall: 14.1% (48,000/340,333) - 6 rows/sec\n",
            " ETA: 755.2 min | Cache hit rate: 0.4%\n",
            " Processing batch 25/171 (2000 rows)\n",
            " Batch 25 complete: 7 rows/sec\n",
            " Overall: 14.7% (50,000/340,333) - 6 rows/sec\n",
            " ETA: 748.3 min | Cache hit rate: 0.5%\n",
            " Checkpoint: 50,000 rows saved\n",
            " Processing batch 26/171 (2000 rows)\n",
            " Batch 26 complete: 7 rows/sec\n",
            " Overall: 15.3% (52,000/340,333) - 6 rows/sec\n",
            " ETA: 742.7 min | Cache hit rate: 0.5%\n",
            " Processing batch 27/171 (2000 rows)\n",
            " Batch 27 complete: 6 rows/sec\n",
            " Overall: 15.9% (54,000/340,333) - 6 rows/sec\n",
            " ETA: 739.7 min | Cache hit rate: 0.5%\n",
            " Processing batch 28/171 (2000 rows)\n",
            " Batch 28 complete: 7 rows/sec\n",
            " Overall: 16.5% (56,000/340,333) - 6 rows/sec\n",
            " ETA: 731.3 min | Cache hit rate: 0.6%\n",
            " Processing batch 29/171 (2000 rows)\n",
            " Batch 29 complete: 6 rows/sec\n",
            " Overall: 17.0% (58,000/340,333) - 6 rows/sec\n",
            " ETA: 726.8 min | Cache hit rate: 0.5%\n",
            " Processing batch 30/171 (2000 rows)\n",
            " Batch 30 complete: 6 rows/sec\n",
            " Overall: 17.6% (60,000/340,333) - 6 rows/sec\n",
            " ETA: 725.3 min | Cache hit rate: 0.5%\n",
            " Checkpoint: 60,000 rows saved\n",
            " Processing batch 31/171 (2000 rows)\n",
            " Batch 31 complete: 6 rows/sec\n",
            " Overall: 18.2% (62,000/340,333) - 6 rows/sec\n",
            " ETA: 722.1 min | Cache hit rate: 0.5%\n",
            " Processing batch 32/171 (2000 rows)\n",
            " Batch 32 complete: 6 rows/sec\n",
            " Overall: 18.8% (64,000/340,333) - 6 rows/sec\n",
            " ETA: 717.2 min | Cache hit rate: 0.5%\n",
            " Processing batch 33/171 (2000 rows)\n",
            " Batch 33 complete: 6 rows/sec\n",
            " Overall: 19.4% (66,000/340,333) - 6 rows/sec\n",
            " ETA: 711.9 min | Cache hit rate: 0.5%\n",
            " Processing batch 34/171 (2000 rows)\n",
            " Batch 34 complete: 7 rows/sec\n",
            " Overall: 20.0% (68,000/340,333) - 6 rows/sec\n",
            " ETA: 705.9 min | Cache hit rate: 0.5%\n",
            " Processing batch 35/171 (2000 rows)\n",
            " Batch 35 complete: 7 rows/sec\n",
            " Overall: 20.6% (70,000/340,333) - 6 rows/sec\n",
            " ETA: 700.1 min | Cache hit rate: 0.5%\n",
            " Checkpoint: 70,000 rows saved\n",
            " Processing batch 36/171 (2000 rows)\n",
            " Batch 36 complete: 6 rows/sec\n",
            " Overall: 21.2% (72,000/340,333) - 6 rows/sec\n",
            " ETA: 696.2 min | Cache hit rate: 0.6%\n",
            " Processing batch 37/171 (2000 rows)\n",
            " Batch 37 complete: 7 rows/sec\n",
            " Overall: 21.7% (74,000/340,333) - 6 rows/sec\n",
            " ETA: 689.5 min | Cache hit rate: 0.9%\n",
            " Processing batch 38/171 (2000 rows)\n",
            " Batch 38 complete: 7 rows/sec\n",
            " Overall: 22.3% (76,000/340,333) - 6 rows/sec\n",
            " ETA: 682.3 min | Cache hit rate: 1.2%\n",
            " Processing batch 39/171 (2000 rows)\n",
            " Batch 39 complete: 7 rows/sec\n",
            " Overall: 22.9% (78,000/340,333) - 6 rows/sec\n",
            " ETA: 674.8 min | Cache hit rate: 1.6%\n",
            " Processing batch 40/171 (2000 rows)\n",
            " Batch 40 complete: 8 rows/sec\n",
            " Overall: 23.5% (80,000/340,333) - 7 rows/sec\n",
            " ETA: 667.1 min | Cache hit rate: 2.0%\n",
            " Checkpoint: 80,000 rows saved\n",
            " Processing batch 41/171 (2000 rows)\n",
            " Batch 41 complete: 8 rows/sec\n",
            " Overall: 24.1% (82,000/340,333) - 7 rows/sec\n",
            " ETA: 658.9 min | Cache hit rate: 2.5%\n",
            " Processing batch 42/171 (2000 rows)\n",
            " Batch 42 complete: 8 rows/sec\n",
            " Overall: 24.7% (84,000/340,333) - 7 rows/sec\n",
            " ETA: 650.5 min | Cache hit rate: 3.0%\n",
            " Processing batch 43/171 (2000 rows)\n",
            " Batch 43 complete: 8 rows/sec\n",
            " Overall: 25.3% (86,000/340,333) - 7 rows/sec\n",
            " ETA: 643.3 min | Cache hit rate: 3.3%\n",
            " Processing batch 44/171 (2000 rows)\n",
            " Batch 44 complete: 8 rows/sec\n",
            " Overall: 25.9% (88,000/340,333) - 7 rows/sec\n",
            " ETA: 635.2 min | Cache hit rate: 3.8%\n",
            " Processing batch 45/171 (2000 rows)\n",
            " Batch 45 complete: 8 rows/sec\n",
            " Overall: 26.4% (90,000/340,333) - 7 rows/sec\n",
            " ETA: 627.9 min | Cache hit rate: 4.1%\n",
            " Checkpoint: 90,000 rows saved\n",
            " Processing batch 46/171 (2000 rows)\n",
            " Batch 46 complete: 8 rows/sec\n",
            " Overall: 27.0% (92,000/340,333) - 7 rows/sec\n",
            " ETA: 620.1 min | Cache hit rate: 4.5%\n",
            " Processing batch 47/171 (2000 rows)\n",
            " Batch 47 complete: 8 rows/sec\n",
            " Overall: 27.6% (94,000/340,333) - 7 rows/sec\n",
            " ETA: 612.6 min | Cache hit rate: 5.0%\n",
            " Processing batch 48/171 (2000 rows)\n",
            " Batch 48 complete: 8 rows/sec\n",
            " Overall: 28.2% (96,000/340,333) - 7 rows/sec\n",
            " ETA: 605.3 min | Cache hit rate: 5.3%\n",
            " Processing batch 49/171 (2000 rows)\n",
            " Batch 49 complete: 8 rows/sec\n",
            " Overall: 28.8% (98,000/340,333) - 7 rows/sec\n",
            " ETA: 598.1 min | Cache hit rate: 5.6%\n",
            " Processing batch 50/171 (2000 rows)\n",
            " Batch 50 complete: 8 rows/sec\n",
            " Overall: 29.4% (100,000/340,333) - 7 rows/sec\n",
            " ETA: 591.0 min | Cache hit rate: 6.0%\n",
            " Checkpoint: 100,000 rows saved\n",
            " Processing batch 51/171 (2000 rows)\n",
            " Batch 51 complete: 8 rows/sec\n",
            " Overall: 30.0% (102,000/340,333) - 7 rows/sec\n",
            " ETA: 584.0 min | Cache hit rate: 6.3%\n",
            " Processing batch 52/171 (2000 rows)\n",
            " Batch 52 complete: 9 rows/sec\n",
            " Overall: 30.6% (104,000/340,333) - 7 rows/sec\n",
            " ETA: 576.8 min | Cache hit rate: 6.7%\n",
            " Processing batch 53/171 (2000 rows)\n",
            " Batch 53 complete: 9 rows/sec\n",
            " Overall: 31.1% (106,000/340,333) - 7 rows/sec\n",
            " ETA: 569.7 min | Cache hit rate: 7.0%\n",
            " Processing batch 54/171 (2000 rows)\n",
            " Batch 54 complete: 9 rows/sec\n",
            " Overall: 31.7% (108,000/340,333) - 7 rows/sec\n",
            " ETA: 562.2 min | Cache hit rate: 7.3%\n",
            " Processing batch 55/171 (2000 rows)\n",
            " Batch 55 complete: 9 rows/sec\n",
            " Overall: 32.3% (110,000/340,333) - 7 rows/sec\n",
            " ETA: 555.3 min | Cache hit rate: 7.7%\n",
            " Checkpoint: 110,000 rows saved\n",
            " Processing batch 56/171 (2000 rows)\n",
            " Batch 56 complete: 8 rows/sec\n",
            " Overall: 32.9% (112,000/340,333) - 7 rows/sec\n",
            " ETA: 548.7 min | Cache hit rate: 8.0%\n",
            " Processing batch 57/171 (2000 rows)\n",
            " Batch 57 complete: 9 rows/sec\n",
            " Overall: 33.5% (114,000/340,333) - 7 rows/sec\n",
            " ETA: 542.1 min | Cache hit rate: 8.3%\n",
            " Processing batch 58/171 (2000 rows)\n",
            " Batch 58 complete: 9 rows/sec\n",
            " Overall: 34.1% (116,000/340,333) - 7 rows/sec\n",
            " ETA: 535.5 min | Cache hit rate: 8.6%\n",
            " Processing batch 59/171 (2000 rows)\n",
            " Batch 59 complete: 9 rows/sec\n",
            " Overall: 34.7% (118,000/340,333) - 7 rows/sec\n",
            " ETA: 529.0 min | Cache hit rate: 8.9%\n",
            " Processing batch 60/171 (2000 rows)\n",
            " Batch 60 complete: 9 rows/sec\n",
            " Overall: 35.3% (120,000/340,333) - 7 rows/sec\n",
            " ETA: 522.6 min | Cache hit rate: 9.2%\n",
            " Checkpoint: 120,000 rows saved\n",
            " Processing batch 61/171 (2000 rows)\n",
            " Batch 61 complete: 9 rows/sec\n",
            " Overall: 35.8% (122,000/340,333) - 7 rows/sec\n",
            " ETA: 516.3 min | Cache hit rate: 9.5%\n",
            " Processing batch 62/171 (2000 rows)\n",
            " Batch 62 complete: 9 rows/sec\n",
            " Overall: 36.4% (124,000/340,333) - 7 rows/sec\n",
            " ETA: 510.1 min | Cache hit rate: 9.7%\n",
            " Processing batch 63/171 (2000 rows)\n",
            " Batch 63 complete: 9 rows/sec\n",
            " Overall: 37.0% (126,000/340,333) - 7 rows/sec\n",
            " ETA: 504.0 min | Cache hit rate: 10.0%\n",
            " Processing batch 64/171 (2000 rows)\n",
            " Batch 64 complete: 9 rows/sec\n",
            " Overall: 37.6% (128,000/340,333) - 7 rows/sec\n",
            " ETA: 497.8 min | Cache hit rate: 10.2%\n",
            " Processing batch 65/171 (2000 rows)\n",
            " Batch 65 complete: 9 rows/sec\n",
            " Overall: 38.2% (130,000/340,333) - 7 rows/sec\n",
            " ETA: 491.7 min | Cache hit rate: 10.4%\n",
            " Checkpoint: 130,000 rows saved\n",
            " Processing batch 66/171 (2000 rows)\n",
            " Batch 66 complete: 9 rows/sec\n",
            " Overall: 38.8% (132,000/340,333) - 7 rows/sec\n",
            " ETA: 485.4 min | Cache hit rate: 10.7%\n",
            " Processing batch 67/171 (2000 rows)\n",
            " Batch 67 complete: 9 rows/sec\n",
            " Overall: 39.4% (134,000/340,333) - 7 rows/sec\n",
            " ETA: 479.6 min | Cache hit rate: 11.0%\n",
            " Processing batch 68/171 (2000 rows)\n",
            " Batch 68 complete: 9 rows/sec\n",
            " Overall: 40.0% (136,000/340,333) - 7 rows/sec\n",
            " ETA: 473.4 min | Cache hit rate: 11.2%\n",
            " Processing batch 69/171 (2000 rows)\n",
            " Batch 69 complete: 9 rows/sec\n",
            " Overall: 40.5% (138,000/340,333) - 7 rows/sec\n",
            " ETA: 467.5 min | Cache hit rate: 11.4%\n",
            " Processing batch 70/171 (2000 rows)\n",
            " Batch 70 complete: 9 rows/sec\n",
            " Overall: 41.1% (140,000/340,333) - 7 rows/sec\n",
            " ETA: 461.7 min | Cache hit rate: 11.6%\n",
            " Checkpoint: 140,000 rows saved\n",
            " Processing batch 71/171 (2000 rows)\n",
            " Batch 71 complete: 9 rows/sec\n",
            " Overall: 41.7% (142,000/340,333) - 7 rows/sec\n",
            " ETA: 456.1 min | Cache hit rate: 11.8%\n",
            " Processing batch 72/171 (2000 rows)\n",
            " Batch 72 complete: 9 rows/sec\n",
            " Overall: 42.3% (144,000/340,333) - 7 rows/sec\n",
            " ETA: 450.2 min | Cache hit rate: 12.0%\n",
            " Processing batch 73/171 (2000 rows)\n",
            " Batch 73 complete: 9 rows/sec\n",
            " Overall: 42.9% (146,000/340,333) - 7 rows/sec\n",
            " ETA: 444.5 min | Cache hit rate: 12.3%\n",
            " Processing batch 74/171 (2000 rows)\n",
            " Batch 74 complete: 9 rows/sec\n",
            " Overall: 43.5% (148,000/340,333) - 7 rows/sec\n",
            " ETA: 438.9 min | Cache hit rate: 12.5%\n",
            " Processing batch 75/171 (2000 rows)\n",
            " Batch 75 complete: 9 rows/sec\n",
            " Overall: 44.1% (150,000/340,333) - 7 rows/sec\n",
            " ETA: 433.3 min | Cache hit rate: 12.7%\n",
            " Checkpoint: 150,000 rows saved\n",
            " Processing batch 76/171 (2000 rows)\n",
            " Batch 76 complete: 9 rows/sec\n",
            " Overall: 44.7% (152,000/340,333) - 7 rows/sec\n",
            " ETA: 427.7 min | Cache hit rate: 12.8%\n",
            " Processing batch 77/171 (2000 rows)\n",
            " Batch 77 complete: 9 rows/sec\n",
            " Overall: 45.2% (154,000/340,333) - 7 rows/sec\n",
            " ETA: 422.3 min | Cache hit rate: 13.0%\n",
            " Processing batch 78/171 (2000 rows)\n",
            " Batch 78 complete: 9 rows/sec\n",
            " Overall: 45.8% (156,000/340,333) - 7 rows/sec\n",
            " ETA: 416.9 min | Cache hit rate: 13.2%\n",
            " Processing batch 79/171 (2000 rows)\n",
            " Batch 79 complete: 9 rows/sec\n",
            " Overall: 46.4% (158,000/340,333) - 7 rows/sec\n",
            " ETA: 411.5 min | Cache hit rate: 13.4%\n",
            " Processing batch 80/171 (2000 rows)\n",
            " Batch 80 complete: 9 rows/sec\n",
            " Overall: 47.0% (160,000/340,333) - 7 rows/sec\n",
            " ETA: 406.2 min | Cache hit rate: 13.6%\n",
            " Checkpoint: 160,000 rows saved\n",
            " Processing batch 81/171 (2000 rows)\n",
            " Batch 81 complete: 9 rows/sec\n",
            " Overall: 47.6% (162,000/340,333) - 7 rows/sec\n",
            " ETA: 400.9 min | Cache hit rate: 13.8%\n",
            " Processing batch 82/171 (2000 rows)\n",
            " Batch 82 complete: 9 rows/sec\n",
            " Overall: 48.2% (164,000/340,333) - 7 rows/sec\n",
            " ETA: 395.7 min | Cache hit rate: 13.9%\n",
            " Processing batch 83/171 (2000 rows)\n",
            " Batch 83 complete: 9 rows/sec\n",
            " Overall: 48.8% (166,000/340,333) - 7 rows/sec\n",
            " ETA: 390.5 min | Cache hit rate: 14.1%\n",
            " Processing batch 84/171 (2000 rows)\n",
            " Batch 84 complete: 9 rows/sec\n",
            " Overall: 49.4% (168,000/340,333) - 7 rows/sec\n",
            " ETA: 385.1 min | Cache hit rate: 14.3%\n",
            " Processing batch 85/171 (2000 rows)\n",
            " Batch 85 complete: 9 rows/sec\n",
            " Overall: 50.0% (170,000/340,333) - 7 rows/sec\n",
            " ETA: 380.0 min | Cache hit rate: 14.4%\n",
            " Checkpoint: 170,000 rows saved\n",
            " Processing batch 86/171 (2000 rows)\n",
            " Batch 86 complete: 9 rows/sec\n",
            " Overall: 50.5% (172,000/340,333) - 7 rows/sec\n",
            " ETA: 374.8 min | Cache hit rate: 14.6%\n",
            " Processing batch 87/171 (2000 rows)\n",
            " Batch 87 complete: 9 rows/sec\n",
            " Overall: 51.1% (174,000/340,333) - 8 rows/sec\n",
            " ETA: 369.5 min | Cache hit rate: 14.8%\n",
            " Processing batch 88/171 (2000 rows)\n",
            " Batch 88 complete: 9 rows/sec\n",
            " Overall: 51.7% (176,000/340,333) - 8 rows/sec\n",
            " ETA: 364.3 min | Cache hit rate: 14.9%\n",
            " Processing batch 89/171 (2000 rows)\n",
            " Batch 89 complete: 9 rows/sec\n",
            " Overall: 52.3% (178,000/340,333) - 8 rows/sec\n",
            " ETA: 359.2 min | Cache hit rate: 15.0%\n",
            " Processing batch 90/171 (2000 rows)\n",
            " Batch 90 complete: 9 rows/sec\n",
            " Overall: 52.9% (180,000/340,333) - 8 rows/sec\n",
            " ETA: 354.0 min | Cache hit rate: 15.2%\n",
            " Checkpoint: 180,000 rows saved\n",
            " Processing batch 91/171 (2000 rows)\n",
            " Batch 91 complete: 9 rows/sec\n",
            " Overall: 53.5% (182,000/340,333) - 8 rows/sec\n",
            " ETA: 349.0 min | Cache hit rate: 15.4%\n",
            " Processing batch 92/171 (2000 rows)\n",
            " Batch 92 complete: 9 rows/sec\n",
            " Overall: 54.1% (184,000/340,333) - 8 rows/sec\n",
            " ETA: 343.9 min | Cache hit rate: 15.5%\n",
            " Processing batch 93/171 (2000 rows)\n",
            " Batch 93 complete: 9 rows/sec\n",
            " Overall: 54.7% (186,000/340,333) - 8 rows/sec\n",
            " ETA: 339.0 min | Cache hit rate: 15.6%\n",
            " Processing batch 94/171 (2000 rows)\n",
            " Batch 94 complete: 9 rows/sec\n",
            " Overall: 55.2% (188,000/340,333) - 8 rows/sec\n",
            " ETA: 334.1 min | Cache hit rate: 15.8%\n",
            " Processing batch 95/171 (2000 rows)\n",
            " Batch 95 complete: 9 rows/sec\n",
            " Overall: 55.8% (190,000/340,333) - 8 rows/sec\n",
            " ETA: 329.2 min | Cache hit rate: 15.9%\n",
            " Checkpoint: 190,000 rows saved\n",
            " Processing batch 96/171 (2000 rows)\n",
            " Batch 96 complete: 9 rows/sec\n",
            " Overall: 56.4% (192,000/340,333) - 8 rows/sec\n",
            " ETA: 324.3 min | Cache hit rate: 16.0%\n",
            " Processing batch 97/171 (2000 rows)\n",
            " Batch 97 complete: 9 rows/sec\n",
            " Overall: 57.0% (194,000/340,333) - 8 rows/sec\n",
            " ETA: 319.3 min | Cache hit rate: 16.1%\n",
            " Processing batch 98/171 (2000 rows)\n",
            " Batch 98 complete: 9 rows/sec\n",
            " Overall: 57.6% (196,000/340,333) - 8 rows/sec\n",
            " ETA: 314.5 min | Cache hit rate: 16.2%\n",
            " Processing batch 99/171 (2000 rows)\n",
            " Batch 99 complete: 9 rows/sec\n",
            " Overall: 58.2% (198,000/340,333) - 8 rows/sec\n",
            " ETA: 309.6 min | Cache hit rate: 16.3%\n",
            " Processing batch 100/171 (2000 rows)\n",
            " Batch 100 complete: 9 rows/sec\n",
            " Overall: 58.8% (200,000/340,333) - 8 rows/sec\n",
            " ETA: 304.8 min | Cache hit rate: 16.4%\n",
            " Checkpoint: 200,000 rows saved\n",
            " Processing batch 101/171 (2000 rows)\n",
            " Batch 101 complete: 9 rows/sec\n",
            " Overall: 59.4% (202,000/340,333) - 8 rows/sec\n",
            " ETA: 300.1 min | Cache hit rate: 16.6%\n",
            " Processing batch 102/171 (2000 rows)\n",
            " Batch 102 complete: 9 rows/sec\n",
            " Overall: 59.9% (204,000/340,333) - 8 rows/sec\n",
            " ETA: 295.3 min | Cache hit rate: 16.7%\n",
            " Processing batch 103/171 (2000 rows)\n",
            " Batch 103 complete: 9 rows/sec\n",
            " Overall: 60.5% (206,000/340,333) - 8 rows/sec\n",
            " ETA: 290.5 min | Cache hit rate: 16.8%\n",
            " Processing batch 104/171 (2000 rows)\n",
            " Batch 104 complete: 9 rows/sec\n",
            " Overall: 61.1% (208,000/340,333) - 8 rows/sec\n",
            " ETA: 285.7 min | Cache hit rate: 16.9%\n",
            " Processing batch 105/171 (2000 rows)\n",
            " Batch 105 complete: 9 rows/sec\n",
            " Overall: 61.7% (210,000/340,333) - 8 rows/sec\n",
            " ETA: 281.0 min | Cache hit rate: 17.0%\n",
            " Checkpoint: 210,000 rows saved\n",
            " Processing batch 106/171 (2000 rows)\n",
            " Batch 106 complete: 9 rows/sec\n",
            " Overall: 62.3% (212,000/340,333) - 8 rows/sec\n",
            " ETA: 276.3 min | Cache hit rate: 17.1%\n",
            " Processing batch 107/171 (2000 rows)\n",
            " Batch 107 complete: 9 rows/sec\n",
            " Overall: 62.9% (214,000/340,333) - 8 rows/sec\n",
            " ETA: 271.6 min | Cache hit rate: 17.3%\n",
            " Processing batch 108/171 (2000 rows)\n",
            " Batch 108 complete: 9 rows/sec\n",
            " Overall: 63.5% (216,000/340,333) - 8 rows/sec\n",
            " ETA: 266.9 min | Cache hit rate: 17.4%\n",
            " Processing batch 109/171 (2000 rows)\n",
            " Batch 109 complete: 9 rows/sec\n",
            " Overall: 64.1% (218,000/340,333) - 8 rows/sec\n",
            " ETA: 262.3 min | Cache hit rate: 17.5%\n",
            " Processing batch 110/171 (2000 rows)\n",
            " Batch 110 complete: 9 rows/sec\n",
            " Overall: 64.6% (220,000/340,333) - 8 rows/sec\n",
            " ETA: 257.6 min | Cache hit rate: 17.6%\n",
            " Checkpoint: 220,000 rows saved\n",
            " Processing batch 111/171 (2000 rows)\n",
            " Batch 111 complete: 9 rows/sec\n",
            " Overall: 65.2% (222,000/340,333) - 8 rows/sec\n",
            " ETA: 253.0 min | Cache hit rate: 17.7%\n",
            " Processing batch 112/171 (2000 rows)\n",
            " Batch 112 complete: 10 rows/sec\n",
            " Overall: 65.8% (224,000/340,333) - 8 rows/sec\n",
            " ETA: 248.4 min | Cache hit rate: 17.8%\n",
            " Processing batch 113/171 (2000 rows)\n",
            " Batch 113 complete: 9 rows/sec\n",
            " Overall: 66.4% (226,000/340,333) - 8 rows/sec\n",
            " ETA: 243.8 min | Cache hit rate: 17.9%\n",
            " Processing batch 114/171 (2000 rows)\n",
            " Batch 114 complete: 9 rows/sec\n",
            " Overall: 67.0% (228,000/340,333) - 8 rows/sec\n",
            " ETA: 239.3 min | Cache hit rate: 18.0%\n",
            " Processing batch 115/171 (2000 rows)\n",
            " Batch 115 complete: 9 rows/sec\n",
            " Overall: 67.6% (230,000/340,333) - 8 rows/sec\n",
            " ETA: 234.8 min | Cache hit rate: 18.1%\n",
            " Checkpoint: 230,000 rows saved\n",
            " Processing batch 116/171 (2000 rows)\n",
            " Batch 116 complete: 9 rows/sec\n",
            " Overall: 68.2% (232,000/340,333) - 8 rows/sec\n",
            " ETA: 230.3 min | Cache hit rate: 18.2%\n",
            " Processing batch 117/171 (2000 rows)\n",
            " Batch 117 complete: 9 rows/sec\n",
            " Overall: 68.8% (234,000/340,333) - 8 rows/sec\n",
            " ETA: 225.8 min | Cache hit rate: 18.2%\n",
            " Processing batch 118/171 (2000 rows)\n",
            " Batch 118 complete: 9 rows/sec\n",
            " Overall: 69.3% (236,000/340,333) - 8 rows/sec\n",
            " ETA: 221.3 min | Cache hit rate: 18.3%\n",
            " Processing batch 119/171 (2000 rows)\n",
            " Batch 119 complete: 9 rows/sec\n",
            " Overall: 69.9% (238,000/340,333) - 8 rows/sec\n",
            " ETA: 216.8 min | Cache hit rate: 18.4%\n",
            " Processing batch 120/171 (2000 rows)\n",
            " Batch 120 complete: 9 rows/sec\n",
            " Overall: 70.5% (240,000/340,333) - 8 rows/sec\n",
            " ETA: 212.3 min | Cache hit rate: 18.4%\n",
            " Checkpoint: 240,000 rows saved\n",
            " Processing batch 121/171 (2000 rows)\n",
            " Batch 121 complete: 9 rows/sec\n",
            " Overall: 71.1% (242,000/340,333) - 8 rows/sec\n",
            " ETA: 207.9 min | Cache hit rate: 18.5%\n",
            " Processing batch 122/171 (2000 rows)\n",
            " Batch 122 complete: 9 rows/sec\n",
            " Overall: 71.7% (244,000/340,333) - 8 rows/sec\n",
            " ETA: 203.4 min | Cache hit rate: 18.6%\n",
            " Processing batch 123/171 (2000 rows)\n",
            " Batch 123 complete: 9 rows/sec\n",
            " Overall: 72.3% (246,000/340,333) - 8 rows/sec\n",
            " ETA: 198.9 min | Cache hit rate: 18.7%\n",
            " Processing batch 124/171 (2000 rows)\n",
            " Batch 124 complete: 9 rows/sec\n",
            " Overall: 72.9% (248,000/340,333) - 8 rows/sec\n",
            " ETA: 194.5 min | Cache hit rate: 18.8%\n",
            " Processing batch 125/171 (2000 rows)\n",
            " Batch 125 complete: 9 rows/sec\n",
            " Overall: 73.5% (250,000/340,333) - 8 rows/sec\n",
            " ETA: 190.1 min | Cache hit rate: 18.8%\n",
            " Checkpoint: 250,000 rows saved\n",
            " Processing batch 126/171 (2000 rows)\n",
            " Batch 126 complete: 10 rows/sec\n",
            " Overall: 74.0% (252,000/340,333) - 8 rows/sec\n",
            " ETA: 185.7 min | Cache hit rate: 18.9%\n",
            " Processing batch 127/171 (2000 rows)\n",
            " Batch 127 complete: 9 rows/sec\n",
            " Overall: 74.6% (254,000/340,333) - 8 rows/sec\n",
            " ETA: 181.3 min | Cache hit rate: 19.0%\n",
            " Processing batch 128/171 (2000 rows)\n",
            " Batch 128 complete: 9 rows/sec\n",
            " Overall: 75.2% (256,000/340,333) - 8 rows/sec\n",
            " ETA: 177.0 min | Cache hit rate: 19.1%\n",
            " Processing batch 129/171 (2000 rows)\n",
            " Batch 129 complete: 10 rows/sec\n",
            " Overall: 75.8% (258,000/340,333) - 8 rows/sec\n",
            " ETA: 172.5 min | Cache hit rate: 19.1%\n",
            " Processing batch 130/171 (2000 rows)\n",
            " Batch 130 complete: 9 rows/sec\n",
            " Overall: 76.4% (260,000/340,333) - 8 rows/sec\n",
            " ETA: 168.1 min | Cache hit rate: 19.3%\n",
            " Checkpoint: 260,000 rows saved\n",
            " Processing batch 131/171 (2000 rows)\n",
            " Batch 131 complete: 9 rows/sec\n",
            " Overall: 77.0% (262,000/340,333) - 8 rows/sec\n",
            " ETA: 163.8 min | Cache hit rate: 19.4%\n",
            " Processing batch 132/171 (2000 rows)\n",
            " Batch 132 complete: 10 rows/sec\n",
            " Overall: 77.6% (264,000/340,333) - 8 rows/sec\n",
            " ETA: 159.4 min | Cache hit rate: 19.5%\n",
            " Processing batch 133/171 (2000 rows)\n",
            " Batch 133 complete: 10 rows/sec\n",
            " Overall: 78.2% (266,000/340,333) - 8 rows/sec\n",
            " ETA: 155.0 min | Cache hit rate: 19.6%\n",
            " Processing batch 134/171 (2000 rows)\n",
            " Batch 134 complete: 10 rows/sec\n",
            " Overall: 78.7% (268,000/340,333) - 8 rows/sec\n",
            " ETA: 150.7 min | Cache hit rate: 19.7%\n",
            " Processing batch 135/171 (2000 rows)\n",
            " Batch 135 complete: 10 rows/sec\n",
            " Overall: 79.3% (270,000/340,333) - 8 rows/sec\n",
            " ETA: 146.3 min | Cache hit rate: 19.8%\n",
            " Checkpoint: 270,000 rows saved\n",
            " Processing batch 136/171 (2000 rows)\n",
            " Batch 136 complete: 10 rows/sec\n",
            " Overall: 79.9% (272,000/340,333) - 8 rows/sec\n",
            " ETA: 142.0 min | Cache hit rate: 20.0%\n",
            " Processing batch 137/171 (2000 rows)\n",
            " Batch 137 complete: 10 rows/sec\n",
            " Overall: 80.5% (274,000/340,333) - 8 rows/sec\n",
            " ETA: 137.7 min | Cache hit rate: 20.1%\n",
            " Processing batch 138/171 (2000 rows)\n",
            " Batch 138 complete: 10 rows/sec\n",
            " Overall: 81.1% (276,000/340,333) - 8 rows/sec\n",
            " ETA: 133.3 min | Cache hit rate: 20.2%\n",
            " Processing batch 139/171 (2000 rows)\n",
            " Batch 139 complete: 10 rows/sec\n",
            " Overall: 81.7% (278,000/340,333) - 8 rows/sec\n",
            " ETA: 129.0 min | Cache hit rate: 20.3%\n",
            " Processing batch 140/171 (2000 rows)\n",
            " Batch 140 complete: 10 rows/sec\n",
            " Overall: 82.3% (280,000/340,333) - 8 rows/sec\n",
            " ETA: 124.7 min | Cache hit rate: 20.4%\n",
            " Checkpoint: 280,000 rows saved\n",
            " Processing batch 141/171 (2000 rows)\n",
            " Batch 141 complete: 10 rows/sec\n",
            " Overall: 82.9% (282,000/340,333) - 8 rows/sec\n",
            " ETA: 120.5 min | Cache hit rate: 20.5%\n",
            " Processing batch 142/171 (2000 rows)\n",
            " Batch 142 complete: 10 rows/sec\n",
            " Overall: 83.4% (284,000/340,333) - 8 rows/sec\n",
            " ETA: 116.2 min | Cache hit rate: 20.6%\n",
            " Processing batch 143/171 (2000 rows)\n",
            " Batch 143 complete: 10 rows/sec\n",
            " Overall: 84.0% (286,000/340,333) - 8 rows/sec\n",
            " ETA: 111.9 min | Cache hit rate: 20.7%\n",
            " Processing batch 144/171 (2000 rows)\n",
            " Batch 144 complete: 10 rows/sec\n",
            " Overall: 84.6% (288,000/340,333) - 8 rows/sec\n",
            " ETA: 107.7 min | Cache hit rate: 20.8%\n",
            " Processing batch 145/171 (2000 rows)\n",
            " Batch 145 complete: 10 rows/sec\n",
            " Overall: 85.2% (290,000/340,333) - 8 rows/sec\n",
            " ETA: 103.5 min | Cache hit rate: 20.9%\n",
            " Checkpoint: 290,000 rows saved\n",
            " Processing batch 146/171 (2000 rows)\n",
            " Batch 146 complete: 10 rows/sec\n",
            " Overall: 85.8% (292,000/340,333) - 8 rows/sec\n",
            " ETA: 99.2 min | Cache hit rate: 21.0%\n",
            " Processing batch 147/171 (2000 rows)\n",
            " Batch 147 complete: 10 rows/sec\n",
            " Overall: 86.4% (294,000/340,333) - 8 rows/sec\n",
            " ETA: 95.0 min | Cache hit rate: 21.1%\n",
            " Processing batch 148/171 (2000 rows)\n",
            " Batch 148 complete: 10 rows/sec\n",
            " Overall: 87.0% (296,000/340,333) - 8 rows/sec\n",
            " ETA: 90.8 min | Cache hit rate: 21.2%\n",
            " Processing batch 149/171 (2000 rows)\n",
            " Batch 149 complete: 10 rows/sec\n",
            " Overall: 87.6% (298,000/340,333) - 8 rows/sec\n",
            " ETA: 86.6 min | Cache hit rate: 21.3%\n",
            " Processing batch 150/171 (2000 rows)\n",
            " Batch 150 complete: 10 rows/sec\n",
            " Overall: 88.1% (300,000/340,333) - 8 rows/sec\n",
            " ETA: 82.5 min | Cache hit rate: 21.4%\n",
            " Checkpoint: 300,000 rows saved\n",
            " Processing batch 151/171 (2000 rows)\n",
            " Batch 151 complete: 10 rows/sec\n",
            " Overall: 88.7% (302,000/340,333) - 8 rows/sec\n",
            " ETA: 78.3 min | Cache hit rate: 21.5%\n",
            " Processing batch 152/171 (2000 rows)\n",
            " Batch 152 complete: 10 rows/sec\n",
            " Overall: 89.3% (304,000/340,333) - 8 rows/sec\n",
            " ETA: 74.1 min | Cache hit rate: 21.5%\n",
            " Processing batch 153/171 (2000 rows)\n",
            " Batch 153 complete: 10 rows/sec\n",
            " Overall: 89.9% (306,000/340,333) - 8 rows/sec\n",
            " ETA: 70.0 min | Cache hit rate: 21.6%\n",
            " Processing batch 154/171 (2000 rows)\n",
            " Batch 154 complete: 10 rows/sec\n",
            " Overall: 90.5% (308,000/340,333) - 8 rows/sec\n",
            " ETA: 65.8 min | Cache hit rate: 21.7%\n",
            " Processing batch 155/171 (2000 rows)\n",
            " Batch 155 complete: 10 rows/sec\n",
            " Overall: 91.1% (310,000/340,333) - 8 rows/sec\n",
            " ETA: 61.7 min | Cache hit rate: 21.8%\n",
            " Checkpoint: 310,000 rows saved\n",
            " Processing batch 156/171 (2000 rows)\n",
            " Batch 156 complete: 10 rows/sec\n",
            " Overall: 91.7% (312,000/340,333) - 8 rows/sec\n",
            " ETA: 57.6 min | Cache hit rate: 21.9%\n",
            " Processing batch 157/171 (2000 rows)\n",
            " Batch 157 complete: 10 rows/sec\n",
            " Overall: 92.3% (314,000/340,333) - 8 rows/sec\n",
            " ETA: 53.4 min | Cache hit rate: 22.0%\n",
            " Processing batch 158/171 (2000 rows)\n",
            " Batch 158 complete: 10 rows/sec\n",
            " Overall: 92.9% (316,000/340,333) - 8 rows/sec\n",
            " ETA: 49.3 min | Cache hit rate: 22.1%\n",
            " Processing batch 159/171 (2000 rows)\n",
            " Batch 159 complete: 10 rows/sec\n",
            " Overall: 93.4% (318,000/340,333) - 8 rows/sec\n",
            " ETA: 45.2 min | Cache hit rate: 22.1%\n",
            " Processing batch 160/171 (2000 rows)\n",
            " Batch 160 complete: 10 rows/sec\n",
            " Overall: 94.0% (320,000/340,333) - 8 rows/sec\n",
            " ETA: 41.1 min | Cache hit rate: 22.2%\n",
            " Checkpoint: 320,000 rows saved\n",
            " Processing batch 161/171 (2000 rows)\n",
            " Batch 161 complete: 10 rows/sec\n",
            " Overall: 94.6% (322,000/340,333) - 8 rows/sec\n",
            " ETA: 37.1 min | Cache hit rate: 22.3%\n",
            " Processing batch 162/171 (2000 rows)\n",
            " Batch 162 complete: 10 rows/sec\n",
            " Overall: 95.2% (324,000/340,333) - 8 rows/sec\n",
            " ETA: 33.0 min | Cache hit rate: 22.4%\n",
            " Processing batch 163/171 (2000 rows)\n",
            " Batch 163 complete: 10 rows/sec\n",
            " Overall: 95.8% (326,000/340,333) - 8 rows/sec\n",
            " ETA: 28.9 min | Cache hit rate: 22.5%\n",
            " Processing batch 164/171 (2000 rows)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "from google.colab import files\n",
        "import io\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import gc\n",
        "import os\n",
        "from datetime import datetime\n",
        "import csv\n",
        "import asyncio\n",
        "import aiohttp\n",
        "from collections import defaultdict\n",
        "import hashlib\n",
        "\n",
        "class FastOTXVerifier:\n",
        "    def __init__(self, api_key, max_workers=20):\n",
        "        self.api_key = api_key\n",
        "        self.base_url = \"https://otx.alienvault.com/api/v1\"\n",
        "        self.headers = {\n",
        "            'X-OTX-API-KEY': api_key,\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "        self.max_workers = max_workers\n",
        "\n",
        "        # Aggressive caching\n",
        "        self.domain_cache = {}\n",
        "        self.ip_cache = {}\n",
        "        self.cache_hits = 0\n",
        "        self.cache_misses = 0\n",
        "\n",
        "        # Rate limiting - more aggressive\n",
        "        self.request_semaphore = threading.Semaphore(15)  # 15 concurrent requests\n",
        "        self.last_requests = []\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "        # Session pool\n",
        "        self.session_pool = []\n",
        "        for _ in range(max_workers):\n",
        "            session = requests.Session()\n",
        "            session.headers.update(self.headers)\n",
        "            self.session_pool.append(session)\n",
        "        self.session_index = 0\n",
        "\n",
        "    def get_session(self):\n",
        "        \"\"\"Get session from pool\"\"\"\n",
        "        with self.lock:\n",
        "            session = self.session_pool[self.session_index]\n",
        "            self.session_index = (self.session_index + 1) % len(self.session_pool)\n",
        "            return session\n",
        "\n",
        "    def rate_limit(self):\n",
        "        \"\"\"Smart rate limiting\"\"\"\n",
        "        with self.lock:\n",
        "            now = time.time()\n",
        "            # Keep only requests from last second\n",
        "            self.last_requests = [t for t in self.last_requests if now - t < 1.0]\n",
        "\n",
        "            # If we've made too many requests, wait\n",
        "            if len(self.last_requests) >= 10:  # 10 requests per second max\n",
        "                sleep_time = 1.0 - (now - self.last_requests[0])\n",
        "                if sleep_time > 0:\n",
        "                    time.sleep(sleep_time)\n",
        "\n",
        "            self.last_requests.append(now)\n",
        "\n",
        "    def get_cache_key(self, item_type, item):\n",
        "        \"\"\"Generate cache key\"\"\"\n",
        "        return f\"{item_type}:{hashlib.md5(str(item).encode()).hexdigest()}\"\n",
        "\n",
        "    def check_domain_fast(self, domain):\n",
        "        \"\"\"Fast domain check with caching\"\"\"\n",
        "        try:\n",
        "            if not domain or str(domain).lower() in ['nan', 'null', '', 'none']:\n",
        "                return {\"status\": \"skipped\", \"pulse_count\": 0, \"threats\": []}\n",
        "\n",
        "            # Clean domain\n",
        "            domain = str(domain).replace('http://', '').replace('https://', '').split('/')[0].strip().lower()\n",
        "            if not domain:\n",
        "                return {\"status\": \"skipped\", \"pulse_count\": 0, \"threats\": []}\n",
        "\n",
        "            # Check cache first\n",
        "            cache_key = self.get_cache_key('domain', domain)\n",
        "            if cache_key in self.domain_cache:\n",
        "                self.cache_hits += 1\n",
        "                return self.domain_cache[cache_key]\n",
        "\n",
        "            self.cache_misses += 1\n",
        "\n",
        "            # Rate limit\n",
        "            self.rate_limit()\n",
        "\n",
        "            # Make request\n",
        "            session = self.get_session()\n",
        "            url = f\"{self.base_url}/indicators/domain/{domain}/general\"\n",
        "\n",
        "            with self.request_semaphore:\n",
        "                response = session.get(url, timeout=5)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                result = self.analyze_domain_response(data)\n",
        "            elif response.status_code == 404:\n",
        "                result = {\"status\": \"safe\", \"pulse_count\": 0, \"threats\": []}\n",
        "            else:\n",
        "                result = {\"status\": \"error\", \"pulse_count\": 0, \"threats\": []}\n",
        "\n",
        "            # Cache result\n",
        "            self.domain_cache[cache_key] = result\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            result = {\"status\": \"error\", \"pulse_count\": 0, \"threats\": []}\n",
        "            # Cache errors too to avoid retrying\n",
        "            cache_key = self.get_cache_key('domain', domain)\n",
        "            self.domain_cache[cache_key] = result\n",
        "            return result\n",
        "\n",
        "    def check_ip_fast(self, ip):\n",
        "        \"\"\"Fast IP check with caching\"\"\"\n",
        "        try:\n",
        "            if not ip or str(ip).lower() in ['nan', 'null', '', 'none']:\n",
        "                return {\"status\": \"skipped\", \"pulse_count\": 0, \"threats\": []}\n",
        "\n",
        "            ip = str(ip).strip()\n",
        "            if not self.is_valid_ip(ip):\n",
        "                return {\"status\": \"skipped\", \"pulse_count\": 0, \"threats\": []}\n",
        "\n",
        "            # Check cache first\n",
        "            cache_key = self.get_cache_key('ip', ip)\n",
        "            if cache_key in self.ip_cache:\n",
        "                self.cache_hits += 1\n",
        "                return self.ip_cache[cache_key]\n",
        "\n",
        "            self.cache_misses += 1\n",
        "\n",
        "            # Rate limit\n",
        "            self.rate_limit()\n",
        "\n",
        "            # Make request\n",
        "            session = self.get_session()\n",
        "            url = f\"{self.base_url}/indicators/IPv4/{ip}/general\"\n",
        "\n",
        "            with self.request_semaphore:\n",
        "                response = session.get(url, timeout=5)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                result = self.analyze_ip_response(data)\n",
        "            elif response.status_code == 404:\n",
        "                result = {\"status\": \"safe\", \"pulse_count\": 0, \"threats\": []}\n",
        "            else:\n",
        "                result = {\"status\": \"error\", \"pulse_count\": 0, \"threats\": []}\n",
        "\n",
        "            # Cache result\n",
        "            self.ip_cache[cache_key] = result\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            result = {\"status\": \"error\", \"pulse_count\": 0, \"threats\": []}\n",
        "            # Cache errors too\n",
        "            cache_key = self.get_cache_key('ip', ip)\n",
        "            self.ip_cache[cache_key] = result\n",
        "            return result\n",
        "\n",
        "    def is_valid_ip(self, ip):\n",
        "        \"\"\"Fast IP validation\"\"\"\n",
        "        try:\n",
        "            parts = ip.split('.')\n",
        "            return len(parts) == 4 and all(0 <= int(part) <= 255 for part in parts)\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def analyze_domain_response(self, data):\n",
        "        \"\"\"Fast domain analysis\"\"\"\n",
        "        pulse_count = data.get('pulse_info', {}).get('count', 0)\n",
        "\n",
        "        if pulse_count == 0:\n",
        "            return {\"status\": \"safe\", \"pulse_count\": 0, \"threats\": []}\n",
        "\n",
        "        # Quick threat detection\n",
        "        pulses = data.get('pulse_info', {}).get('pulses', [])[:5]  # Only check first 5\n",
        "\n",
        "        malicious_keywords = ['malware', 'phishing', 'botnet', 'ransomware', 'trojan']\n",
        "        suspicious_keywords = ['suspicious', 'scan', 'probe']\n",
        "\n",
        "        threats = []\n",
        "        threat_level = \"safe\"\n",
        "\n",
        "        for pulse in pulses:\n",
        "            pulse_name = pulse.get('name', '').lower()\n",
        "            tags = ' '.join(pulse.get('tags', [])).lower()\n",
        "\n",
        "            # Quick keyword matching\n",
        "            for keyword in malicious_keywords:\n",
        "                if keyword in pulse_name or keyword in tags:\n",
        "                    threat_level = \"malicious\"\n",
        "                    threats.append(keyword)\n",
        "                    break\n",
        "\n",
        "            if threat_level != \"malicious\":\n",
        "                for keyword in suspicious_keywords:\n",
        "                    if keyword in pulse_name or keyword in tags:\n",
        "                        threat_level = \"suspicious\"\n",
        "                        threats.append(keyword)\n",
        "                        break\n",
        "\n",
        "        return {\n",
        "            \"status\": threat_level,\n",
        "            \"pulse_count\": pulse_count,\n",
        "            \"threats\": list(set(threats))\n",
        "        }\n",
        "\n",
        "    def analyze_ip_response(self, data):\n",
        "        \"\"\"Fast IP analysis\"\"\"\n",
        "        pulse_count = data.get('pulse_info', {}).get('count', 0)\n",
        "\n",
        "        if pulse_count == 0:\n",
        "            return {\"status\": \"safe\", \"pulse_count\": 0, \"threats\": []}\n",
        "\n",
        "        # Quick threat detection\n",
        "        pulses = data.get('pulse_info', {}).get('pulses', [])[:5]  # Only check first 5\n",
        "\n",
        "        malicious_keywords = ['malware', 'botnet', 'c2', 'command', 'control']\n",
        "        suspicious_keywords = ['suspicious', 'scan', 'probe']\n",
        "\n",
        "        threats = []\n",
        "        threat_level = \"safe\"\n",
        "\n",
        "        for pulse in pulses:\n",
        "            pulse_name = pulse.get('name', '').lower()\n",
        "            tags = ' '.join(pulse.get('tags', [])).lower()\n",
        "\n",
        "            # Quick keyword matching\n",
        "            for keyword in malicious_keywords:\n",
        "                if keyword in pulse_name or keyword in tags:\n",
        "                    threat_level = \"malicious\"\n",
        "                    threats.append(keyword)\n",
        "                    break\n",
        "\n",
        "            if threat_level != \"malicious\":\n",
        "                for keyword in suspicious_keywords:\n",
        "                    if keyword in pulse_name or keyword in tags:\n",
        "                        threat_level = \"suspicious\"\n",
        "                        threats.append(keyword)\n",
        "                        break\n",
        "\n",
        "        return {\n",
        "            \"status\": threat_level,\n",
        "            \"pulse_count\": pulse_count,\n",
        "            \"threats\": list(set(threats))\n",
        "        }\n",
        "\n",
        "    def get_cache_stats(self):\n",
        "        \"\"\"Get cache statistics\"\"\"\n",
        "        total = self.cache_hits + self.cache_misses\n",
        "        hit_rate = (self.cache_hits / total * 100) if total > 0 else 0\n",
        "        return {\n",
        "            'hits': self.cache_hits,\n",
        "            'misses': self.cache_misses,\n",
        "            'hit_rate': hit_rate,\n",
        "            'domain_cache_size': len(self.domain_cache),\n",
        "            'ip_cache_size': len(self.ip_cache)\n",
        "        }\n",
        "\n",
        "def process_row_fast(verifier, row_data):\n",
        "    \"\"\"Process single row with optimizations\"\"\"\n",
        "    domain = str(row_data.get('Full Domain', '')).strip() if pd.notna(row_data.get('Full Domain')) else \"\"\n",
        "    ip_address = str(row_data.get('IP Address', '')).strip() if pd.notna(row_data.get('IP Address')) else \"\"\n",
        "\n",
        "    # Quick skip for obviously empty data\n",
        "    if not domain and not ip_address:\n",
        "        return {\n",
        "            'original_domain': domain,\n",
        "            'ip_address': ip_address,\n",
        "            'domain_status': 'skipped',\n",
        "            'ip_status': 'skipped',\n",
        "            'overall_risk': 'UNKNOWN'\n",
        "        }\n",
        "\n",
        "    # Process concurrently\n",
        "    domain_result = verifier.check_domain_fast(domain) if domain else {\"status\": \"skipped\", \"pulse_count\": 0, \"threats\": []}\n",
        "    ip_result = verifier.check_ip_fast(ip_address) if ip_address else {\"status\": \"skipped\", \"pulse_count\": 0, \"threats\": []}\n",
        "\n",
        "    # Determine overall risk quickly\n",
        "    if domain_result['status'] == 'malicious' or ip_result['status'] == 'malicious':\n",
        "        overall_risk = 'HIGH'\n",
        "    elif domain_result['status'] == 'suspicious' or ip_result['status'] == 'suspicious':\n",
        "        overall_risk = 'MEDIUM'\n",
        "    elif domain_result['status'] == 'safe' and ip_result['status'] == 'safe':\n",
        "        overall_risk = 'LOW'\n",
        "    else:\n",
        "        overall_risk = 'UNKNOWN'\n",
        "\n",
        "    return {\n",
        "        'original_domain': domain,\n",
        "        'main_domain': row_data.get('Main Domain', ''),\n",
        "        'subdomain': row_data.get('Subdomain', ''),\n",
        "        'ip_address': ip_address,\n",
        "        'domain_status': domain_result['status'],\n",
        "        'domain_threats': ', '.join(domain_result['threats']),\n",
        "        'domain_pulse_count': domain_result['pulse_count'],\n",
        "        'ip_status': ip_result['status'],\n",
        "        'ip_threats': ', '.join(ip_result['threats']),\n",
        "        'ip_pulse_count': ip_result['pulse_count'],\n",
        "        'overall_risk': overall_risk\n",
        "    }\n",
        "\n",
        "def process_batch_parallel(verifier, batch_data, batch_num, total_batches):\n",
        "    \"\"\"Process batch with maximum parallelism\"\"\"\n",
        "    print(f\" Processing batch {batch_num}/{total_batches} ({len(batch_data)} rows)\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Use ThreadPoolExecutor for parallel processing\n",
        "    with ThreadPoolExecutor(max_workers=verifier.max_workers) as executor:\n",
        "        # Submit all rows in batch\n",
        "        future_to_row = {\n",
        "            executor.submit(process_row_fast, verifier, row): idx\n",
        "            for idx, (_, row) in enumerate(batch_data.iterrows())\n",
        "        }\n",
        "\n",
        "        # Collect results as they complete\n",
        "        for future in as_completed(future_to_row):\n",
        "            try:\n",
        "                result = future.result(timeout=10)\n",
        "                results.append(result)\n",
        "            except Exception as e:\n",
        "                # Add error result\n",
        "                results.append({\n",
        "                    'original_domain': 'ERROR',\n",
        "                    'ip_address': 'ERROR',\n",
        "                    'domain_status': 'error',\n",
        "                    'ip_status': 'error',\n",
        "                    'overall_risk': 'ERROR'\n",
        "                })\n",
        "\n",
        "    return results\n",
        "\n",
        "def main():\n",
        "    print(\" ULTRA-FAST Domain & IP Security Verifier\")\n",
        "    print(\" Optimized for 300k-400k rows with aggressive caching\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Configuration for speed\n",
        "    BATCH_SIZE = 2000  # Larger batches\n",
        "    MAX_WORKERS = 20   # More workers\n",
        "    CHECKPOINT_INTERVAL = 10000  # Less frequent checkpoints\n",
        "\n",
        "    # Get API key\n",
        "    api_key = input(\"Enter your AlienVault OTX API Key: \").strip()\n",
        "    if not api_key:\n",
        "        print(\" API key is required!\")\n",
        "        return\n",
        "\n",
        "    # Speed configuration\n",
        "    print(\"\\n Speed Configuration:\")\n",
        "    print(f\" Concurrent workers: {MAX_WORKERS}\")\n",
        "    print(f\" Batch size: {BATCH_SIZE}\")\n",
        "    print(f\" Aggressive caching: Enabled\")\n",
        "    print(f\" Rate limit: 10 req/sec per worker\")\n",
        "\n",
        "    # Initialize fast verifier\n",
        "    verifier = FastOTXVerifier(api_key, max_workers=MAX_WORKERS)\n",
        "\n",
        "    # Upload CSV\n",
        "    print(f\"\\n Upload your CSV file:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\" No file uploaded!\")\n",
        "        return\n",
        "\n",
        "    filename = list(uploaded.keys())[0]\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Fast CSV loading\n",
        "        print(\" Fast loading CSV...\")\n",
        "        df = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
        "\n",
        "        total_rows = len(df)\n",
        "        print(f\" Loaded {total_rows:,} rows in {time.time()-start_time:.1f}s\")\n",
        "\n",
        "        # Display column information\n",
        "        print(f\"\\n CSV Columns detected:\")\n",
        "        for i, col in enumerate(df.columns):\n",
        "            print(f\"  {i+1}. '{col}'\")\n",
        "\n",
        "        # Validate columns - FIXED: Check for correct column names\n",
        "        required_columns = ['Full Domain', 'IP Address']\n",
        "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "\n",
        "        if missing_columns:\n",
        "            print(f\" Required columns missing: {missing_columns}\")\n",
        "            print(f\" Available columns: {list(df.columns)}\")\n",
        "            return\n",
        "\n",
        "        print(\" All required columns found!\")\n",
        "\n",
        "        # Memory optimization\n",
        "        df = df.fillna('')  # Replace NaN with empty strings\n",
        "\n",
        "        # Pre-analysis for duplicates\n",
        "        print(\" Analyzing data for optimization...\")\n",
        "        unique_domains = df['Full Domain'].nunique()\n",
        "        unique_ips = df['IP Address'].nunique()\n",
        "\n",
        "        duplicate_savings = total_rows - unique_domains - unique_ips\n",
        "        print(f\" Unique domains: {unique_domains:,}\")\n",
        "        print(f\" Unique IPs: {unique_ips:,}\")\n",
        "        print(f\" Cache will save ~{duplicate_savings:,} API calls\")\n",
        "\n",
        "        # Calculate batches\n",
        "        total_batches = (total_rows + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "        print(f\"\\n Processing {total_batches} batches of {BATCH_SIZE} rows each\")\n",
        "\n",
        "        # Setup output\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        output_file = f\"fast_verification_results_{timestamp}.csv\"\n",
        "\n",
        "        all_results = []\n",
        "        processed_rows = 0\n",
        "\n",
        "        # Process all batches\n",
        "        for batch_num in range(1, total_batches + 1):\n",
        "            batch_start_time = time.time()\n",
        "\n",
        "            start_idx = (batch_num - 1) * BATCH_SIZE\n",
        "            end_idx = min(start_idx + BATCH_SIZE, total_rows)\n",
        "            batch_data = df.iloc[start_idx:end_idx]\n",
        "\n",
        "            # Process batch in parallel\n",
        "            batch_results = process_batch_parallel(verifier, batch_data, batch_num, total_batches)\n",
        "            all_results.extend(batch_results)\n",
        "            processed_rows += len(batch_results)\n",
        "\n",
        "            # Performance stats\n",
        "            batch_time = time.time() - batch_start_time\n",
        "            batch_rate = len(batch_results) / batch_time if batch_time > 0 else 0\n",
        "\n",
        "            # Overall progress\n",
        "            progress = (processed_rows / total_rows) * 100\n",
        "            elapsed = time.time() - start_time\n",
        "            overall_rate = processed_rows / elapsed if elapsed > 0 else 0\n",
        "            eta = (total_rows - processed_rows) / overall_rate if overall_rate > 0 else 0\n",
        "\n",
        "            # Cache stats\n",
        "            cache_stats = verifier.get_cache_stats()\n",
        "\n",
        "            print(f\" Batch {batch_num} complete: {batch_rate:.0f} rows/sec\")\n",
        "            print(f\" Overall: {progress:.1f}% ({processed_rows:,}/{total_rows:,}) - {overall_rate:.0f} rows/sec\")\n",
        "            print(f\" ETA: {eta/60:.1f} min | Cache hit rate: {cache_stats['hit_rate']:.1f}%\")\n",
        "\n",
        "            # Quick checkpoint for very large files\n",
        "            if processed_rows % CHECKPOINT_INTERVAL == 0:\n",
        "                checkpoint_file = f\"checkpoint_{timestamp}.csv\"\n",
        "                pd.DataFrame(all_results).to_csv(checkpoint_file, index=False)\n",
        "                print(f\" Checkpoint: {processed_rows:,} rows saved\")\n",
        "\n",
        "            # Memory management\n",
        "            del batch_data, batch_results\n",
        "            gc.collect()\n",
        "\n",
        "        # Final results\n",
        "        total_time = time.time() - start_time\n",
        "        final_rate = processed_rows / total_time\n",
        "\n",
        "        print(f\"\\n PROCESSING COMPLETE!\")\n",
        "        print(f\" Total time: {total_time/60:.1f} minutes\")\n",
        "        print(f\" Average rate: {final_rate:.0f} rows/minute\")\n",
        "        print(f\" Cache efficiency: {cache_stats['hit_rate']:.1f}% hit rate\")\n",
        "\n",
        "        # Quick summary\n",
        "        results_df = pd.DataFrame(all_results)\n",
        "        risk_counts = results_df['overall_risk'].value_counts()\n",
        "\n",
        "        print(f\"\\n SECURITY SUMMARY:\")\n",
        "        for risk, count in risk_counts.items():\n",
        "            emoji = {\"HIGH\": \"\", \"MEDIUM\": \"\", \"LOW\": \"\"}.get(risk, \"\")\n",
        "            print(f\"{emoji} {risk}: {count:,} ({count/total_rows*100:.1f}%)\")\n",
        "\n",
        "        # Save and download\n",
        "        print(f\"\\n Saving results...\")\n",
        "        results_df.to_csv(output_file, index=False)\n",
        "\n",
        "        print(f\" Downloading {output_file}...\")\n",
        "        files.download(output_file)\n",
        "\n",
        "        print(f\"\\n VERIFICATION COMPLETE!\")\n",
        "        print(f\" Processed {total_rows:,} rows in {total_time/60:.1f} minutes\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error: {str(e)}\")\n",
        "        # Emergency save\n",
        "        if 'all_results' in locals() and all_results:\n",
        "            emergency_file = f\"emergency_save_{int(time.time())}.csv\"\n",
        "            pd.DataFrame(all_results).to_csv(emergency_file, index=False)\n",
        "            files.download(emergency_file)\n",
        "            print(f\" Emergency save: {len(all_results)} results\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1gfSghh/KN55IZ/7416dE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}